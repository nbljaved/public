#+title: Software Design by Example
#+subtitle: A Tool-Based Introduction with Python
#+auto_tangle: t
#+startup: overview
https://third-bit.com/sdxpy/

* Chapter 2: Objects and Classes
:PROPERTIES:
:ref: https://third-bit.com/sdxpy/oop/
:header-args:python: :session ch2 :results output :async yes
:END:
** Theory
*** Intro

#+CAPTION: Python class example (with inheritance)
#+begin_src python
import math

class Shape:
    def __init__(self, name):
        self.name = name

    def perimeter(self):
        raise NotImplementedError

    def area(self):
        raise NotImplementedError

    def density(self, weight):
        return weight / self.area()

class Square(Shape):
    def __init__(self, name, side):
        super().__init__(name)
        self.side = side

    def perimeter(self):
        return 4 * self.side

    def area(self):
        return self.side ** 2

class Circle(Shape):
    def __init__(self, name, radius):
        super().__init__(name)
        self.radius = radius

    def perimeter(self):
        return 2 * math.pi * self.radius

    def area(self):
        return math.pi * (self.radius ** 2)

#+end_src

#+RESULTS:

<<making constructors>>
- NOTE :: How in the subclasses' constructor we have to *manually*
  1. mention the arguments (to ~__init__~) and
  2. call the ~super~ method

Since =Square= and =Circle= have the same methods, we can use them interchangibly.
This is called *polymorphism*.
It reduces cognitive load by allowing the people using related things to ignore their differences:

#+begin_src python
examples = [Square("sq", 3), Circle("ci", 2)]
for thing in examples:
    n = thing.name
    p = thing.perimeter()
    a = thing.area()
    d = thing.density(5)
    print(f"{n} has perimeter {p:.2f}, area {a:.2f} and density {d:.2f}")
#+end_src

#+RESULTS:
: sq has perimeter 12.00, area 9.00 and density 0.56
: ci has perimeter 12.57, area 12.57 and density 0.40

*** Using dictionary to implement polymorphism
:PROPERTIES:
:ID:       c5c68489-11e0-4960-bb9b-f003d127ac63
:header-args:python: :session ch2 :results output :eval no :async yes
:END:

#+CAPTION: Concept map for implementing objects and classes
[[file:images/Chapter_2:_Objects_and_Classes/2025-08-17_15-49-47_screenshot.png]]

#+NAME: my-classes
#+begin_src python :noweb yes :eval yes
from pprint import pprint

<<my-class-methods>>
<<my-class-constructors>>
<<generic-make-method>>

MyShape = {
    # methods
    "density": shape_density,# this method will be inherited
    # metadata
    "_classname": "MyShape", # obj._classname will tell the class of an object
    "_new": make_my_shape,
    "_parent": None, # obj._parent will is parent class of this object's class
}

MySquare = {
    "perimeter": square_perimeter,
    "area": square_area,
    # metadata
    "_new": make_my_square,
    "_classname": "MySquare",
    "_parent": MyShape,
}

MyCircle = {
    "perimeter": circle_perimeter,
    "area": circle_area,
    # metadata
    "_new": make_my_circle,
    "_classname": "MyCircle",
    "_parent": MyShape,
}

pprint(make(MyCircle, 'cute circle', radius=2))
pprint(make(MySquare, 'stupid square', side=4))
#+end_src

#+RESULTS: my-classes
#+begin_example
{'class': {'_classname': 'MyCircle',
           '_new': <function make_my_circle at 0x7ed8a19349a0>,
           '_parent': {'_classname': 'MyShape',
                       '_new': <function make_my_shape at 0x7ed8a19345e0>,
                       '_parent': None,
                       'density': <function shape_density at 0x7ed8a19351c0>},
           'area': <function circle_area at 0x7ed8a1934fe0>,
           'perimeter': <function circle_perimeter at 0x7ed8a1934ae0>},
 'name': 'cute circle',
 'radius': 2}
{'class': {'_classname': 'MySquare',
           '_new': <function make_my_square at 0x7ed8a19360c0>,
           '_parent': {'_classname': 'MyShape',
                       '_new': <function make_my_shape at 0x7ed8a19345e0>,
                       '_parent': None,
                       'density': <function shape_density at 0x7ed8a19351c0>},
           'area': <function square_area at 0x7ed8a1937e20>,
           'perimeter': <function square_perimeter at 0x7ed8a1937240>},
 'name': 'stupid square',
 'side': 4}
#+end_example

Our classes only define:
+ ~_classname~ :: the class' name
+ ~_new~ :: their constructor
  - NOTE :: requires the Class, which in turn requires this constructor, but it works :)
+ ~_parent~ :: reference to their parent class.
  Therefore, we also /automatically/ inherit the method's of the parent class.
+ the class' methods

#+CAPTION: Constructors for MySquare and MyCircle
#+NAME: my-class-constructors
#+begin_src python :noweb yes
def make(cls, *args, **kwargs):
    """
    Generic make function
    """
    return cls['_new'](*args, **kwargs)

def make_my_shape(name):
    return {
        "name": name,
        "class": MyShape,
    }

def make_my_square(name, side):
    # calling the super method
    # assigning new attributes
    #
    # MySquare overwrites the 'class' key
    # (we use '|' to combine 2 dicts)
    return make(MyShape, name) | {
        "side": side,
        "class": MySquare,
    }

def make_my_circle(name, radius):
    return make(MyShape, name) | {
        "radius": radius,
        "class": MyCircle,
    }
#+end_src

Each class constructor only defines:
+ ~class~ :: reference to its class
+ its attributes (_all manual_)
  See [[making constructors]]
  - NOTE :: Here we are mentioning all the attributes, even those of the parent class.
    - This is a *manual* step.
      When making the constructor for some class, we need to /THINK/ about its parent class and the arguments of that class' constructor.
      (Also, Python can only have one ~__init__~  method)

#+CAPTION: Methods for MyShape, MySquare, MyCircle
#+NAME: my-class-methods
#+begin_src python
# In the below function the 'self' represents the obj
def shape_density(self, weight):
    return weight / call_method(self, "area")

def square_perimeter(self):
    return 4 * self['side']

def square_area(self):
    return self['side'] ** 2

def circle_perimeter(self):
    return 2 * math.pi * self['radius']

def circle_area(self):
    return math.pi * (self['radius'] ** 2)
#+end_src

- Notice how some 'Shape' object would call its 'density' method:
  ~obj["density"](obj, weight)~

  which is very similar to:
  ~obj.density(weight)~ (here the 'self' object passed is implicit)

  - Also, not the definition of the 'shape_density' function.
    Its arguments are the same as that of a normal class' methods.

*** call_method implementation

The call_method implementation is important as that is what ties all this together.

It has to look like ~call_method(object, method_name, method_arguments ...)~

#+NAME: call_method
#+begin_src python
def call_method(obj, method_name: str, *method_args, **method_kwargs):
    def find_method(obj, method_name):
        "Returns the method or None (if unable to find)"
        cls = obj['class']
        while cls:
            if method_name in cls:
                return cls[method_name]
            cls = cls['_parent']
        return None

    method = find_method(obj, method_name)
    print(f'calling method: {method.__name__}, with args: {method_args}, with kwargs: {method_kwargs}')
    return method(obj, *method_args, **method_kwargs)
#+end_src

#+RESULTS: call_method

#+begin_src python
examples = [make(MySquare, "sq", 3), make(MyCircle, "ci", 2)]
for ex in examples:
    n = ex["name"]
    d = call_method(ex, "density", 5)
    print(f"{n}: {d:.2f}")
    print()
#+end_src

#+RESULTS:
: calling method: shape_density, with args: (5,), with kwargs: {}
: calling method: square_area, with args: (), with kwargs: {}
: sq: 0.56
:
: calling method: shape_density, with args: (5,), with kwargs: {}
: calling method: circle_area, with args: (), with kwargs: {}
: ci: 0.40

** Questions
*** Class methods and static methods

*Q.* Explain the differences between class methods and static methods
*A.* [[https://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python][StackOverflow answer]]

*Q.* Implement both using dictionaries.

We need changes:
- in the ~call_method~ function
- how we store a method
  - we have to mention the function implementation as well as the type of method it is, one of:
    - normal method ~def (self, ...)~
    - classmethod ~def (cls, ...)~
    - staticmethod ~def (...)~ (No self or cls)

* Chapter 3: Finding duplicate files
:PROPERTIES:
:ref: https://third-bit.com/sdxpy/dup/
:header-args:python: :session ch3 :results output :async yes
:GPTEL_TOPIC: chapter-3:-finding-duplicate-files
:END:
** Summary
:PROPERTIES:
:ID:       ba463b34-d27b-4adc-a0ee-9021cd241ad9
:END:

- A hash function:
  * creates a fixed-size value from an arbitrary sequence of bytes.
  * has deterministic output, but it is not easy to predict.
  * if good, has evenly distributed output.
- A large cryptographic hash can be used to uniquely identify a file's contents.

#+caption: summarizes the key ideas in this chapter, the most important of which is that SOME ALGORITHMS ARE INTRINSICALLY BETTER THAN OTHERS.
[[file:images/Chapter_3:_Finding_duplicate_files/2025-08-18_15-14-09_screenshot.png]]

** Theory
*** Setup

#+caption: Prepare files
#+begin_src sh
mkdir -p ./src/ch3/files

$(
    cd ./src/ch3/files

    echo aaa > a1.txt
    echo aaa > a2.txt
    echo aaa > a3.txt
    echo bb > b1.txt
    echo bb > b2.txt
    echo c > c1.txt
)

ls ./src/ch3/files
#+end_src

#+RESULTS:
| a1.txt |
| a2.txt |
| a3.txt |
| b1.txt |
| b2.txt |
| c1.txt |

#+name: filenames
#+begin_src python
from pprint import pprint

filenames = [
    './src/ch3/files/a1.txt',
    './src/ch3/files/a2.txt',
    './src/ch3/files/a3.txt',
    './src/ch3/files/b1.txt',
    './src/ch3/files/b2.txt',
    './src/ch3/files/c1.txt'
]
#+end_src

*** Naive approach
If we have /N/ files.
To find all duplicate files, it will take us N-1 + N-2 + ... + 1 = O(N^2) comparisions.
Each of the comparision also involves expensive byte-by-byte comparision on files the _might_ be equal.

#+name: same-bytes
#+begin_src python
def same_bytes(file1: str, file2: str, log=False) -> bool:
    bytes1 = open(file1, 'rb').read()
    bytes2 = open(file2, 'rb').read()
    if log:
        print(bytes1)
        print(bytes2)
    return bytes1 == bytes2
#+end_src

#+begin_src python :noweb yes
<<same-bytes>>
print(same_bytes('./src/ch3/files/a1.txt', './src/ch3/files/c1.txt', log=True))
#+end_src

#+RESULTS:
: b'aaa\n'
: b'c\n'
: False

- Note ::  that the files are opened in binary mode using ~"rb"~ instead of the
  usual ~"r"~.
  - This tells Python to read the bytes exactly as they are rather than trying to convert them to characters.

#+name: naive
#+caption: O(n^2) algorithm
#+begin_src python :noweb yes
<<filenames>>
<<same-bytes>>

def find_duplicates(filenames):
    duplicates = []
    n = len(filenames)
    for i in range(n):
        file1 = filenames[i]
        for j in range(i+1, n):
            file2 = filenames[j]
            if same_bytes(file1, file2):
                duplicates.append((file1, file2))
    return duplicates

pprint(find_duplicates(filenames))
#+end_src

#+RESULTS: naive
: [('./src/ch3/files/a1.txt', './src/ch3/files/a2.txt'),
:  ('./src/ch3/files/a1.txt', './src/ch3/files/a3.txt'),
:  ('./src/ch3/files/a2.txt', './src/ch3/files/a3.txt'),
:  ('./src/ch3/files/b1.txt', './src/ch3/files/b2.txt')]

*** Hash approach
Instead of comparing every file against every other, let’s process each file once to produce a short identifier that depends only on the file’s contents and then only compare files that have the same identifier, i.e., that /might/ be equal.

If files are evenly divided into =g= groups then each group will contain roughly =N/g= files, so the total work will be roughly =g*(N/g)^2=.

Simplifying, this is =N^2/g= , so as the number of groups grows, and the overall running time should decrease.

- We can use *hashing* to split the files into groups.
  We construct IDs for files by using a /hash function/ to produce a /hash code/.

  Since bytes are just numbers, we can create a very simple hash function by adding up the bytes in a file and taking the remainder module some number (the no. of groups we want).

#+caption: A naive hash function
#+begin_src python
def naive_hash(data):
    return sum(data) % 13

example = bytes('file content', 'utf-8')
print(f'Example: {example}')

id = naive_hash(example)
print(f'id: {id} \n')

for byt in example:
    print(f'byte: {byt}')
print(f'\nsum: {sum(example)}, sum%13 = {sum(example)%13}')
#+end_src

#+RESULTS:
#+begin_example
Example: b'file content'
id: 2

byte: 102
byte: 105
byte: 108
byte: 101
byte: 32
byte: 99
byte: 111
byte: 110
byte: 116
byte: 101
byte: 110
byte: 116

sum: 1211, sum%13 = 2
#+end_example

*** Better hashing
For =g= groups we have to perform =O(N^2/g)= comparisions.
If /g = N/, i.e. for each file its id has _a unique bucket_, then our complexiy becomes =O(N)=.

- NOTE :: We have to read each file at least once anyway, so we can’t possibly do better than =O(N)=.

*Q.* How can we ensure that each unique file winds up in its own group?

*A.* Use a /cryptographic hash function/:

   - The output of such a function is completely deterministic: given the same bytes in the same order, it will always produce the same output.

   - However, the output is distributed like a uniform random variable: each possible output is equally likely, which ensures that files will be evenly distributed between groups.

- SHA256 hashing algorithm :: Given some bytes as input, this function produces a 256-bit hash, which is normally written as a 64-character (as 64x4=256) hexadecimal (4bit) string.

#+name: sha256
#+begin_src python
from hashlib import sha256

# strings must be encoded before hashing
example = bytes('some content', 'utf-8')

output = sha256(example).hexdigest()
print(output)
#+end_src

#+RESULTS: sha256
: 290f493c44f5d63d06b374d0a5abd292fae38b92cab2fae5efefe1b0e9347f56

#+name: duplicates
#+caption: O(n) algorithm
#+begin_src python :noweb yes
from hashlib import sha256
<<filenames>>

def hash_files(filenames):
    groups = dict()
    for filename in filenames:
        data = open(filename, 'rb').read()
        hash_code = sha256(data).hexdigest()
        if hash_code not in groups:
            groups[hash_code] = set()
        groups[hash_code].add(filename)
    return groups

for files in hash_files(filenames).values():
    # each set of files is a duplicate of each other
    print(files)
#+end_src

#+RESULTS: duplicates
: {'./src/ch3/files/a2.txt', './src/ch3/files/a1.txt', './src/ch3/files/a3.txt'}
: {'./src/ch3/files/b2.txt', './src/ch3/files/b1.txt'}
: {'./src/ch3/files/c1.txt'}

*** Birthday problem

The odds that two people share a birthday are 1/365 (ignoring February 29).

The odds that they don’t are therefore \(\frac{365}{365}\)x\(\frac{364}{365}\).
When we add a third person, the odds that nobody share a birthday are \(\frac{365}{365}\)x\(\frac{364}{365}\)x\(\frac{363}{365}\).

If we keep going, there’s a 50% chance of two people sharing a birthday in a group of just 23 people, and a _99.9% chance with 70 people_.

The same math can tell us how many files we need to hash before there’s a 50% chance of a collision with a 256-bit hash. According to Wikipedia, the answer is approximately 4 x 10^38 files.
We’re willing to take that risk.

** Questions
*** Odds of collision
If hashes were only 2 bits long, then the chances of collision with each successive file assuming no previous collision are:

| Number of Files | Odds of Collision |
|-----------------+-------------------|
|               1 |                0% |
|               2 |               25% |
|               3 |               50% |
|               4 |               75% |
|               5 |              100% |

*Q.* A colleague of yours says this means that if we hash four files, there’s only a 75% chance of any collision occurring. What are the actual odds?

*A.* This is _a common confusion_.

2 bits means 4 possible codes.
The odds of collision:
- #files = 1 is 0.
- #files = 2 is 1/4 (second file will collide if it takes the value of the first file)
- #files = 3 is 2/4 (third file will collide if it takes value of either of the previous files)
- #files = 4 is 3/4
- #files >= 5 is 1.

But, if we hash 4 files the odds of them not colliding is = 4/4 x 3/4 x 2/4 x 1/4 = 3/32
Therefore, the odds of their being 1 collision when hashing 4 files = 1 - 3/32 = 0.90625 i.e. 90.625%

*** Streaming

- streaming API ::
  An API that processes data in chunks rather than needing to have all of it in memory at once.
  Streaming APIs usually require handlers for events such as:
  - “start of data”,
  - “next block”, and
  - “end of data”

A streaming API delivers data one piece at a time rather than all at once. Read the documentation for the update method of hashing objects in Python’s [[https://docs.python.org/3/library/hashlib.html][hashing module]] and rewrite the duplicate finder from this chapter to use it.

#+caption: streaming hashing
#+begin_src python :noweb yes
from hashlib import sha256

m = sha256()
# => m = <sha256 _hashlib.HASH object @ 0x71eb1a40c730>

m.update(b"Nobody inspects")
m.update(b" the spammish repetition")

m.digest()
# => b'\x03\x1e\xdd}Ae\x15\x93\xc5\xfe\\\x00o\xa5u+7\xfd\xdf\xf7\xbcN\x84:\xa6\xaf\x0c\x95\x0fK\x94\x06'

m.hexdigest()
# => '031edd7d41651593c5fe5c006fa5752b37fddff7bc4e843aa6af0c950f4b9406'

sha256(b"Nobody inspects the spammish repetition").hexdigest()
# => '031edd7d41651593c5fe5c006fa5752b37fddff7bc4e843aa6af0c950f4b9406'
#+end_src

* Chapter 4: Matching patterns
:PROPERTIES:
:ref: https://third-bit.com/sdxpy/glob/
:header-args:python: :session ch4 :results output :async yes
:header-args:sh: :session ch4-sh :results output :async yes
:END:
** Summary
:PROPERTIES:
:ID:       1e611433-31e2-4a18-800b-903d2eebc169
:END:

- Use globs and regular expressions to match patterns in text.
- Use inheritance to make matchers composable and extensible.
- Simplify code by having objects delegate work to other objects.

- Use the *Null Object pattern* to eliminate special cases in code.

- Use standard refactorings to move code from one working state to another.
- Build and check the parts of your code you are least sure of first to find out if your design will work.

#+caption: summarizes the key ideas in this chapter; we will see the *Null Object* and *Chain of Responsibility* design patterns again.
[[file:images/Chapter_4:_Matching_patterns/2025-08-18_16-54-52_screenshot.png]]

** Theory
*** Setup

#+begin_src sh
mkdir -p ./src/ch4/
#+end_src

*** Simple patterns
:PROPERTIES:
:ID:       f9f26345-73e4-4da9-a36c-0fbb127cf219
:END:

|---------+--------+--------|
| *Pattern* | *Text*   | *Match?* |
|---------+--------+--------|
| abc     | “abc”  | true   |
| ab      | “abc”  | false  |
| abc     | “ab”   | false  |
| *       | ”“     | true   |
| *       | “abc”  | true   |
| a*c     | “abc”  | true   |
| {a,b}   | “a”    | true   |
| {a,b}   | “c”    | false  |
| {a,b}   | “ab”   | false  |
| *{x,y}  | “abcx” | true   |
|---------+--------+--------|

+ Matching is conceptually simple.

  * If the first element of the pattern matches the target string at the current location, we check if the rest of the pattern matches what’s left of the string.

  * If the element doesn’t match the front of the string, or if the rest of the pattern can’t match the rest of the string, matching fails. (This behavior makes globbing different from regular expressions, which can match parts of strings.)

This design makes use of the *Chain of Responsibility* design pattern.
Each matcher matches if it can then asks the next matcher in the chain to try to match the remaining text (Figure 4.2).
Crucially, objects don’t know how long the chain after them is: they just know whom to ask next.

#+caption: matching with Chain of Responsibility
[[file:images/Chapter_4:_Matching_patterns/2025-08-18_17-34-00_screenshot.png]]

- Chain of Responsibility pattern ::
  A design pattern in which each object either handles a request or passes it on to another object.

- Null Object pattern ::
  A design pattern in which a placeholder object is used instead of /None/.

  The placeholder object has the methods of the object usually used, but those methods do 'nothing'.
  - NOTE :: 'nothing' DOESN'T mean that the return value of those methods has to be /None/.

  This pattern saves other code from having to check repeatedly for /None/.

So, we will define a base class ~Pattern~ that will have a method ~match~ which returns boolean indicating if the ~Pattern~ matches some text.
To refer to the next ~Pattern~ we will note that down in its 'rest' attribute.

#+name: base-class
#+caption: base and null object class
#+begin_src python :tangle ./src/ch4/matcher.py :noweb yes
Pattern = type('Pattern')
class Pattern:
    def __init__(self, rest: Pattern | None = None):
        """
        Subclasses of Pattern can take two arguments:
        1. set of characters, a str etc.
        2. rest (the next Pattern)
        """
        # NOTE: How we can pass the argument has None and it is handled by the
        # Null Object pattern
        self.rest : Pattern = rest if rest is not None else Null()

    def match(self, text: str) -> bool:
        """
        Returns boolean indicating if Pattern matches `text`.
        """
        length_of_matched_text = self._match(text, start=0)
        return len(text) == length_of_matched_text

    <<Pattern __eq__>>

class Null(Pattern):
    """
    Null Object Pattern
    Null() is the placeholder object instead of 'None'
    """
    def __init__(self):
        """
        Null objects must be at the end of the matching chain, i.e., their 'rest'
        must be None, so we remove the 'rest' parameter from the class’s
        constructor and pass 'None' up to the parent constructor every time.
        """
        self.rest = None # base case

    def _match(self, text, start):
        """
        Since Null objects don’t match anything, Null._match immediately returns
        whatever starting point it was given.

        Every other matcher can now pass responsibility down the chain without
        having to test whether it’s the last matcher in line or not.
        """
        return start

<<literal pattern>>
<<any pattern>>
<<either pattern>>
#+end_src

- ~Match.rest~ requires every child class to have a helper method called ~_match~ that returns the location from which searching is to continue.

- ~Match.match~ checks whether the entire match reaches the end of the target string and returns True or False as appropriate.

*** Implement Lit(Pattern)

#+caption: literal tests
#+begin_src python :tangle ./src/ch4/test_literal.py
from matcher import *

def test_literal_match_entire_string():
    # /abc/ matches "abc"
    assert Lit("abc").match("abc")

def test_literal_substring_alone_no_match():
    # /ab/ doesn't match "abc"
    assert not Lit("ab").match("abc")

def test_literal_superstring_no_match():
    # /abc/ doesn't match "ab"
    assert not Lit("abc").match("ab")
#+end_src

For the above tests we define the following ~Literal~ pattern:

#+name: literal pattern
#+begin_src python :noweb yes
class Lit(Pattern):
    def __init__(self, chars: str, rest=None):
        super().__init__(rest)
        self.chars = chars

    def _match(self, text: str, start=0):
        end = len(self.chars) + start
        if text[start:end] != self.chars:
            # failed
            #
            # this is the position to next search,
            # (therefore None means failed)
            return None
        # passed
        return self.rest._match(text, start=end)

    <<Lit __eq__>>
#+end_src


#+caption: literal test to make sure chaining is working
#+begin_src python :tangle ./src/ch4/test_literal.py
def test_literal_followed_by_literal_match():
    # /a/+/b/ matches "ab"
    assert Lit("a", Lit("b")).match("ab")

def test_literal_followed_by_literal_no_match():
    # /a/+/b/ doesn't match "ac"
    assert not Lit("a", Lit("b")).match("ac")
#+end_src


#+begin_src sh :async yes
uvx pytest ./src/ch4/test_literal.py
#+end_src

#+RESULTS:
: =============================== test session starts ================================
: platform linux -- Python 3.12.8, pytest-8.4.1, pluggy-1.6.0
: rootdir: /home/nabeel/Documents/public/books/Software Design by Example
: collected 5 items
:
: src/ch4/test_literal.py .....                                                [100%]
:
: ================================ 5 passed in 0.01s =================================

*** Implement Any(Pattern)

#+caption: any's tests
#+begin_src python :tangle ./src/ch4/test_any.py
from matcher import *

def test_any_matches_empty():
    # /*/ matches ""
    assert Any().match("")

def test_any_matches_entire_string():
    # /*/ matches "abc"
    assert Any().match("abc")

def test_any_matches_as_prefix():
    # /*def/ matches "abcdef"
    assert Any(Lit("def")).match("abcdef")

def test_any_matches_as_suffix():
    # /abc*/ matches "abcdef"
    assert Lit("abc", Any()).match("abcdef")

def test_any_matches_interior():
    # /a*c/ matches "abc"
    assert Lit("a", Any(Lit("c"))).match("abc")
#+end_src

Keeping in mind the above tests, we write the following class:

#+name: any pattern
#+begin_src python
class Any(Pattern):
    def __init__(self, rest=None):
        super().__init__(rest)

    def _match(self, text: str, start=0):
        """
        Here '*' can match 0 or more of the characters
        in text[start:].
        Here, we implement lazy matching, '*' will match
        the shortes string so that the 'rest' of Pattern can
        successfully match 'text'.
        """
        n = len(text)
        # length matched = from 0 -> len(text[start:])
        #
        # NOTE1: i=n+1 => * matched the complete text[start:]
        #       and the 'rest' will have to match the empty string.
        for i in range(start, n+1):
            # i the index where 'rest' will start matching from,
            # i.e. 'rest' tries to match text[i:]
            #
            # if self.rest.match(text[i:]):
            #     return n
            # OR
            j = self.rest._match(text, start=i)
            if j == n: # NOTE2: why we test for this (hint we don't rely on 'rest')
                # success
                return n # NOTE3

        # fail
        return None
#+end_src

- NOTE :: In the above code
  - NOTE1
  - NOTE2
  - NOTE3

#+begin_src sh
uvx pytest ./src/ch4/test_any.py
#+end_src

#+RESULTS:
: =============================== test session starts ================================
: platform linux -- Python 3.12.8, pytest-8.4.1, pluggy-1.6.0
: rootdir: /home/nabeel/Documents/public/books/Software Design by Example
: collected 5 items
:
: src/ch4/test_any.py .....                                                    [100%]
:
: ================================ 5 passed in 0.01s =================================

*** Implement Either(Pattern)

#+name: test either
#+begin_src python :tangle ./src/ch4/test_either.py
from matcher import *

def test_either_two_literals_first():
    # /{a,b,c}/ matches "a"
    assert Either(Lit("a"), Lit("b")).match("a")

def test_either_two_literals_not_both():
    # /{a,b,c}/ doesn't match "ab"
    assert not Either(Lit("a"), Lit("b"), Lit("c")).match("ab")

def test_either_followed_by_literal_match():
    # /{a,b,c}d/ matches "cd"
    assert Either(Lit("a"), Lit("b"), Lit("c"), rest=Lit("d")).match("cd")

def test_either_followed_by_literal_no_match():
    # /{a,b,c}d/ doesn't match "cx"
    assert not Either(Lit("a"), Lit("b"), Lit("c"), rest=Lit("d")).match("cx")

def test_either_followed_by_literal_no_match2():
    # /{a,b,cd}d/ matches "cd"
    assert not Either(Lit("a"), Lit("b"), Lit("cd"), rest=Lit("d")).match("cd")

def test_empty_either_empty_literal_match():
    # /{}/ matches ""
    assert Either().match("")

def test_empty_either_literal_match():
    # /{}abc/ matches ""
    assert Either(rest=Lit("abc")).match("abc")

def test_empty_either_literal_no_match():
    # /{}abc/ doesn't match "abd"
    assert not Either().match("abd")
#+end_src

Keeping in mind the above test cases:

#+name: either pattern
#+begin_src python
class Either(Pattern):
    def __init__(self, *patterns, rest=None):
        super().__init__(rest)
        self.patterns = patterns

    def _match(self, text, start=0):
        # NOTE: what if patterns are empty ?
        if not self.patterns:
            return self.rest._match(text, start)

        # Try each pattern
        for pattern in self.patterns:
            j = pattern._match(text, start)
            if j is None:
                continue
            if len(text) == self.rest._match(text, start=j):
                # pass
                return len(text)
        # fail
        return None
#+end_src

#+begin_src sh
uvx pytest ./src/ch4/test_either.py
#+end_src

#+RESULTS:
: =============================== test session starts ================================
: platform linux -- Python 3.12.8, pytest-8.4.1, pluggy-1.6.0
: rootdir: /home/nabeel/Documents/public/books/Software Design by Example
: collected 8 items
:
: src/ch4/test_either.py ........                                              [100%]
:
: ================================ 8 passed in 0.01s =================================

* Chapter 5: Parsing Text
:PROPERTIES:
:ref: https://third-bit.com/sdxpy/parse/
:header-args:python: :session ch5 :results output :async yes :eval no
:header-args:sh: :session ch5-sh :results output :async yes
:END:

** Summary
:PROPERTIES:
:ID:       6e6a4a75-8d34-4f9c-ad32-7cb0d668fd09
:END:

#+caption: Gist
#+begin_example
Tranform text -> list of tokens List[[token_type, token_args]] --parse-> either AST or actual python objects
#+end_example

 - Parsing transforms text that's easy for people to read into objects that are easy for computers to work with.
 - A grammar defines the textual patterns that a parser recognizes.

 - Most parsers tokenize input text and then analyze the tokens.
 - Most parsers need to implement some form of precedence to prioritize different patterns.

 - Operations like addition and function call work just like user-defined functions.
 - Programs can overload built-in operators by defining specially-named methods that are recognized by the compiler or interpreter.

#+caption: Parser concept map
[[file:images/Chapter_5:_Parsing_Text/2025-08-19_16-39-33_screenshot.png]]

** Theory
*** Intro
We constructed objects to match patterns in [[*Chapter 4: Matching patterns][Chapter 4: Matching patterns]], but an expression like ~"2023-*{pdf,txt}"~ is a lot easier to read and write than code like ~Lit("2023-", Any(Either("pdf", "txt")))~.

If we want to use the former, we need a parser to convert those human-readable strings into machine-comprehensible objects.

#+caption: glob grammar that our parse will handle
| *Meaning*                 | *Character* |
|-------------------------+-----------|
| Any literal character c | c         |
| Zero or more characters | *         |
| Alternatives            | {x,y}     |

When we are done, our parser should be able to recognize that =2023-*.{pdf,txt}= means the literal =2023-= , any characters, a literal =.=, and then either a literal =pdf= or a literal =txt=.

*** Tokenizing
:PROPERTIES:
:ID:       13eb27ae-7cbf-4c1d-9671-a4d69fdb3aa2
:END:

Most parsers are written in 2 parts:

1. The first stage groups characters into atoms of text called “tokens“, which are meaningful pieces of text like the digits making up a number or the letters making up a variable name.

2. The second stage of parsing assembles tokens to create an *abstract syntax tree*,
   that represents the structure of what was parsed

#+caption: Stages in parsing pipeline
[[file:images/Chapter_5:_Parsing_Text/2025-08-20_10-08-30_screenshot.png]]

Our grammars token are:
- special characters:
  - ,
  - {
  - }
  - *
- (other characters) a sequence of one or more other characters is a single multi-letter token.

The above classification determines the design of our tokenizer:

1. If a character is not special, then append it to the current literal (if there is one) or start a new literal (if there isn’t).

2. If a character is special, then close the existing literal (if there is one) and create a token for the special character. Note that the =,= character closes a literal but doesn’t produce a token.

Eg: For the string "2023-*{pdf, txt}", out tokenizer will output:
#+begin_src python
[
    ['Lit', '2023-'],
    ['Any'],
    ['EitherStart'],
    ['Lit', 'pdf'],
    ['Lit', 'txt'],
    ['EitherEnd']
]
#+end_src

We use the above structure for our tokens because they represent the pattern and the arguments that pattern takes.

#+name: tokenizer
#+begin_src python :tangle ./src/ch5/tokenizer.py
class Tokenizer():
    def __init__(self):
        self._setup()

    def _setup(self):
        # NOTE: We are defining the class attributes in a function
        #       other than __init__ !
        self.result = []
        self.current = ""

    def _add(self, thing):
        """
        Adds the current thing to the list of tokens.
        Examples of 'thing': ['Any'], ['EitherStart'], ['EitherEnd']

        As a special case, self._add(None) means “add the literal but nothing
        else”
        """
        if len(self.current) > 0:
            self.result.append(['Lit', self.current])
            self.current = ''
        if thing is not None:
            self.result.append(thing)

    def tok(self, text: str):
        """
        Main method of our tokenizer
        """
        # This method calls self._setup() at the start so that the tokenizer can
        # be re-used
        self._setup()

        for c in text:
            if c == '*':
                self._add(['Any'])
            elif c == '{':
                self._add(['EitherStart'])
            elif c == '}':
                self._add(['EitherEnd'])
            elif c == ',':
                self._add(None)
            elif c.isascii():
                self.current += c
            else:
                raise NotImplementedError(f'what is {c} ?')
        # NOTE: We do this to add the final 'current' to the 'result'.
        return self.result
#+end_src

The above class based implementation is very nice:
- we have ~_setup()~ method to *reset* the state of our /current/ and /result/ variables.
- we have ~_add(pattern_name | None)~ to add the literal (before adding the special character, if it exists).

#+name: tokenizer tests
#+begin_src python :tangle ./src/ch5/test_tokenizer.py
from .tokenizer import *

def test_tok_empty_string():
    assert Tokenizer().tok("") == []

def test_tok_any_either():
    assert Tokenizer().tok("*{abc,def}") == [
        ["Any"],
        ["EitherStart"],
        ["Lit", "abc"],
        ["Lit", "def"],
        ["EitherEnd"],
    ]
#+end_src

#+begin_src sh
uvx pytest ./src/ch5/test_tokenizer.py
#+end_src

#+RESULTS:
: ============================================================================ test session starts ============================================================================
: platform linux -- Python 3.12.8, pytest-8.4.1, pluggy-1.6.0
: rootdir: /home/nabeel/Documents/public/books/Software Design by Example
: collected 2 items
:
: src/ch5/test_tokenizer.py ..                                                                                                                                          [100%]
:
: ============================================================================= 2 passed in 0.01s =============================================================================

*** Parsing

We now need to turn the list of tokens into a tree.

Just as we used a class for tokenizing, we will create one for parsing and give it a ~_parse~ method to start things off.
This method doesn’t do any conversion itself.
Instead, it takes a token off the front of the list and figures out which method handles tokens of that kind:

#+begin_src python :tangle ./src/ch5/parser.py
from .tokenizer import *
from ..ch4.matcher import *

class Parser():
    def __init__(self):
        pass

    def parse(self, text: str):
        tokens = Tokenizer().tok(text)
        return self._parse(tokens)

    def _parse(self, tokens):
        if not tokens:
            return Null()

        car = tokens[0]
        cdr = tokens[1:]
        pattern = car[0]
        if pattern == 'Any':
            handler = self._parse_Any
        elif pattern == 'Lit':
            handler = self._parse_Lit
        elif pattern == 'EitherStart':
            handler = self._parse_EitherStart
        else:
            assert False, f'Unknown token type {pattern}'

        return handler(car[1:], cdr)

    def _parse_Any(self, arg, rest_tokens):
        return Any(rest=self._parse(rest_tokens))

    def _parse_Lit(self, arg, rest_tokens):
        text = arg[0]
        return Lit(chars=text, rest=self._parse(rest_tokens))

    def _parse_EitherStart(self, arg, rest_tokens):
        args = [] # with store the options of Either

        for i in range(len(rest_tokens)):
            token = rest_tokens[i]
            if token[0] == 'EitherEnd':
                either_end_index = i
                break
            else:
                pattern = self._parse([token])
                args.append(pattern)

        return Either(*args, rest=self._parse(rest_tokens[either_end_index+1:]))
#+end_src

Now let's write a test to test our implementation of ~Parser~:

#+begin_src python :tangle ./src/ch5/test_parser.py
from .parser import *
from ..ch4.matcher import *

def test_parse_either_two_lit():
    assert Parser().parse("{abc,def}") == Either(
        [Lit("abc"), Lit("def")]
    )
#+end_src

To run the above test, we need to first implement *equal* operation by the ~Pattern~ class.

This test assumes we can compare ~Pattern~ objects using ====, just as we would compare numbers or strings. so we add a ~__eq__~ method to our classes:

#+name: Pattern __eq__
#+begin_src python
def __eq__(self, other):
    return (other is not None
            and self.__class__ == other.__class__
            and self.rest == other.rest)
#+end_src

#+name: Lit __eq__
#+begin_src python
def __eq__(self, other):
    return super().__eq__(other) and (self.chars == other.chars)

#+end_src

In the above Note:
- definition of ~__eq__(self, other)~
- usage of ~__class__~
- usage of ~super().__eq__(other)__~

- Since we’re using inheritance to implement our matchers, we write the check for equality in two parts.

  1. The parent class ~Pattern~ performs the checks that all classes need to perform (in this case, that the objects being compared have the same /concrete class/).

  2. If the child class needs to do any more checking (for example, that the characters in two ~Lit~ objects are the same) it calls up to the parent method first, then adds its own tests.

#+begin_src sh
uvx pytest ./src/ch5/test_parser.py
#+end_src

#+RESULTS:
: ============================================================================ test session starts ============================================================================
: platform linux -- Python 3.12.8, pytest-8.4.1, pluggy-1.6.0
: rootdir: /home/nabeel/Documents/public/books/Software Design by Example
: collected 1 item
:
: src/ch5/test_parser.py .                                                                                                                                              [100%]
:
: ============================================================================= 1 passed in 0.01s =============================================================================
** Questions
*** Nested Lists
*Q.* Write a function that accepts a string representing nested lists containing numbers and returns the actual list. For example, the input [1, [2, [3, 4], 5]] should produce the corresponding Python list.

*A.* convert the text into:
#+begin_src python
# [1, [2, [3, 4], 5]]
# is converted to
[
    ['ListStart'],
    ['Number', '1'],
    ['ListStart'],
    ['Number', '2'],
    ['ListStart'],
    ['Number', '3'],
    ['Number', '4'],
    ['ListEnd'],
    ['Number', '5'],
    ['ListEnd'],
    ['ListEnd'],
]
#+end_src


#+begin_src python :eval yes

def isnumber(text: str) -> bool:
    "Returns True if text represents a integer or decimal number"
    try:
        float(text)
        return True
    except:
        return False

def tokenize(text: str):
    current = ""
    result = []

    def add(token):
        """
        Add current to 'result', then adds 'token' to 'result'
        """
        nonlocal current
        if len(current) > 0:
            result.append(['Number', current])
            current = ''
        if token is not None:
            result.append(token)

    for char in text:
        if char == '[':
            # special tokens like these signal the end of 'current'
            # as well as need to be taken care of themselves
            add(['ListStart'])
        elif char == ']':
            add(['ListEnd'])
        elif char == ',':
            # ',' signals the end of 'current'
            add(None)
        elif char.isspace():
            # if i just ignore it
            # then [2 2] would represent [22],
            # but let's ignore that for now
            continue
        elif isnumber(current + char):
            current += char
        else:
            raise NotImplementedError(f'what is this char: {char} ?')

    add(None)
    return result

expected = [
    ['ListStart'],
    ['Number', '1'],
    ['ListStart'],
    ['Number', '2'],
    ['ListStart'],
    ['Number', '3'],
    ['Number', '4'],
    ['ListEnd'],
    ['Number', '5'],
    ['ListEnd'],
    ['ListEnd'],
]
actual = tokenize('[1, [2, [3, 4], 5]]')
print(actual == expected)
#+end_src

#+RESULTS:
: True

To parse this one has to process the list of tokens and call individual parse helper methods recursively.

*** Simple Arithmetic
*Q.*
Write a function that accepts a string consisting of numbers and the basic arithmetic operations +, -, *, and /, and produces a nested structure showing the operations in the correct order.

For example, 1 + 2 * 3 should produce ["+", 1, ["*", 2, 3]].

*A.* Here operations have a relative ordering, it is NOT simply left to right.

#+begin_src python
# For  "1 + 2 * 3"
# we get
[
    ['Number', '1'],
    ['+'],
    ['Number', '2'],
    ['*'],
    ['Number', '3'],
]
#+end_src

- NOTE :: Maybe we also have brackets "(1 + 2) * 3"

So, there is an ordering:
1. Brackets
2. Division
3. Multiplication
4. Addition
5. Subtraction

- Aim :: Is to convert this list of tokens into an AST, keeping in the above order of operations in mind.

Let's assume our string is:
#+begin_src python
example = '(3 + 3) / -.6 - -10 * 1.0'
#+end_src

In the above 'example', we have operations, numbers, and spaces.
- In this string, spaces shouldn't matter (assuming we have been given valid syntax)
- Therefore, our string only has number stuff(digits, decimal, and negative) and operations

#+name: tokenize arithmetic
#+begin_src python :tangle ./src/ch5/tokenize_arithmetic.py
from pprint import pprint

def tokenize(text: str):
    result = []
    current = ''

    def looking_for_number() -> bool:
        """
        Return true if 'result's last token is an operation
        """
        if not result:
            return False
        last_token = result[-1]
        # check if it is a number
        if last_token[0] == 'Number':
            return

    def add(token):
        nonlocal current
        if len(current) > 0:
            result.append(['Number', current])
            current = ''
        if token:
            result.append(token)

    for i, char in enumerate(text):
        if char == '(':
            add(['('])
        elif char == ')':
            add([')'])
        elif char == '+':
            add(['+'])
        elif char == '*':
            add(['*'])
        elif char == '/':
            add(['/'])
        elif char.isspace():
            continue
        # number can be '0-9',
        elif char.isdigit():
            current += char
        elif char == '.':
            if '.' not in current:
                current += char
            else:
                raise Exception(f'you promised valid string')
        elif char == '-':
            # now either this is the minus operation
            # or negative on a number
            #
            # If 'current' is empty => negative
            # If 'current' is not empty => minus operation
            if current:
                add(['-'])
            else:
                current += '-'
        else:
            raise Exception(f'provided text: {text} is supposed to be valid')

    add(None)
    return result
#+end_src

#+begin_src python :eval yes :noweb yes
<<tokenize arithmetic>>
example = '(3 + 3) / -.6 - -10 * 1.0'

pprint(tokenize(example))
#+end_src

#+RESULTS:
#+begin_example
[['('],
 ['Number', '3'],
 ['+'],
 ['Number', '3'],
 [')'],
 ['/'],
 ['Number', '-.6'],
 ['-'],
 ['Number', '-10'],
 ['*'],
 ['Number', '1.0']]
#+end_example

Now what remains is to convert this list of token into the requested AST.
To do that we will not parse left-to-right, but we will go through the tokens parsing (ruling out) each operation one by one.

#+name: parse arithmetic
#+begin_src python :noweb yes :tangle ./src/ch5/parse_arithmetic.py
from .tokenize_arithmetic import *

<<parsed arithmetic classes>>

<<handle bracket>>

<<handle division>>

<<handle multiplication>>

<<handle addition>>

<<handle subtraction>>

def parse_token(token) -> Parsed:
    if isinstance(token, Parsed):
        return token
    # either is an operation
    # or is a number
    if token[0] == 'Number':
        return Number(value=token[1])

    # token must be an operation
    # We can't parse an operation in isolation
    raise Exception(f'Cannot process operation: {token} in isolation')

def parse(tokens):
    tokens = handle_bracket(tokens)
    tokens = handle_division(tokens)
    tokens = handle_multiplication(tokens)
    tokens = handle_addition(tokens)
    tokens = handle_subtraction(tokens)
    return tokens


#+end_src

#+name: parsed arithmetic classes
#+begin_src python
from dataclasses import dataclass

class Parsed:
    pass

@dataclass
class Number(Parsed):
    value: str

    def __call__(self):
        return self.value

class Operation(Parsed):
    def __call__(self):
        return [self.op, self.left(), self.right()]

@dataclass
class Add(Operation):
    left: Parsed
    right: Parsed
    op: str = '+'

@dataclass
class Subtract(Operation):
    left: Parsed
    right: Parsed
    op: str = '-'

@dataclass
class Multiply(Operation):
    left: Parsed
    right: Parsed
    op: str = '*'

@dataclass
class Divide(Operation):
    left: Parsed
    right: Parsed
    op: str = '*'
#+end_src

#+name: handle bracket
#+begin_src python
def find_bracket_end(tokens, bracket_open_index):
    opened = 1
    for i in range(bracket_open_index+1, len(tokens)):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '(':
            opened += 1
        elif isinstance(token, list) and token[0] == ')':
            opened -= 1
            if opened == 0:
                return i

    raise Exception(f'Could not find corresponding closing bracket for open bracket at index: {bracket_open_index}')

def handle_bracket(tokens):
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '(':
            # we have to first parse everything inside this
            # '(' ... ')' pair
            # to find its bracket end
            j = find_bracket_end(tokens, i)
            tokens[i:j+1] = parse(tokens[i+1:j])
            #
            i = i
        else:
            i += 1
    return tokens
#+end_src

#+name: handle division
#+begin_src python
def handle_division(tokens):
    # a / b / c => (a / b) / c
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '/':
            left, right = tokens[i-1], tokens[i+1]
            tokens[i-1:i+2] = [Divide(left=parse_token(left),
                                     right=parse_token(right))]
            # 3 tokens replaced by 1
            # 0 1 2 3(/) 4
            # 0 1 2=23(/)4 3
            i = i
            continue
        else:
            i += 1

    return tokens
#+end_src

#+name: handle multiplication
#+begin_src python
def handle_multiplication(tokens):
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '*':
            left, right = tokens[i-1], tokens[i+1]
            tokens[i-1:i+2] = [Multiply(left=parse_token(left),
                                       right=parse_token(right))]
            i = i
            continue
        else:
            i += 1
    return tokens
#+end_src

#+name: handle addition
#+begin_src python
def handle_addition(tokens):
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '+':
            left, right = tokens[i-1], tokens[i+1]
            tokens[i-1:i+2] = [Add(left=parse_token(left),
                                  right=parse_token(right))]
            i = i
            continue
        else:
            i += 1
    return tokens
#+end_src

#+name: handle subtraction
#+begin_src python
def handle_subtraction(tokens):
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '-':
            left, right = tokens[i-1], tokens[i+1]
            tokens[i-1:i+2] = [Subtract(left=parse_token(left),
                                       right=parse_token(right))]
            i = i
            continue
        else:
            i += 1
    return tokens
#+end_src

Time to test it now:

#+begin_src python :tangle ./src/ch5/test_arithmetic.py
from .parse_arithmetic import *
from .tokenize_arithmetic import *
from pprint import pprint

def test_tokenize():
    example = '(3 + 3) / -.6 - -10 * 1.0'
    expected = [['('],
                ['Number', '3'],
                ['+'],
                ['Number', '3'],
                [')'],
                ['/'],
                ['Number', '-.6'],
                ['-'],
                ['Number', '-10'],
                ['*'],
                ['Number', '1.0']]
    actual = tokenize(example)
    assert actual == expected

def test_parse():
    example = '(3 + 3) / -.6 - -10 * 1.0'
    tokens = tokenize(example)
    actual = parse(tokens)
    expected = [Subtract(left=Divide(left=Add(left=Number(value='3'),
                                              right=Number(value='3'),
                                              op='+'),
                                     right=Number(value='-.6'),
                                     op='*'),
                         right=Multiply(left=Number(value='-10'),
                                        right=Number(value='1.0'),
                                        op='*'),
                         op='-')]
    assert actual == expected

def test_ast():
    example = '(3 + 3) / -.6 - -10 * 1.0'
    tokens = tokenize(example)
    parsed_list = parse(tokens)
    actual = parsed_list[0]()
    expected = ['-', ['*', ['+', '3', '3'], '-.6'], ['*', '-10', '1.0']]
    assert actual == expected

#+end_src

- NOTE :: ~@dataclass~ also implements equality, good for me

* Chapter 6: Running Tests
:PROPERTIES:
:ref: https://third-bit.com/sdxpy/test/
:header-args:python: :session ch6 :results output :async yes :eval no
:header-args:sh: :session ch6-sh :results output :async yes
:END:
** Summary
:PROPERTIES:
:ID:       09c1b9b4-68ef-4107-8e92-920827954bee
:END:

- Functions are objects you can save in data structures or pass to other functions.

- Python stores local and global variables in dictionary-like structures.

- A unit test performs an operation on a fixture and passes, fails, or produces an error.
- A program can use introspection to find functions and other objects at runtime.

#+caption: Concept map
[[file:images/Chapter_6:/2025-08-21_09-57-47_screenshot.png]]

When reviewing the ideas introduced in this chapter, it’s worth remembering /Clarke’s Third Law/, which states that any sufficiently advanced technology is indistinguishable from magic.

The same is true of programming tricks like *introspection*: the code that finds tests dynamically seems transparent to an expert who _understands that CODE IS DATA_, but can be incomprehensible to a novice.

** Theory
*** Storing and running tests
A function is just an object that we can assign to a variable. We can also store them in lists just like numbers or strings.

#+begin_src python :eval yes
def first():
    print("First")

def second():
    print("Second")

def third():
    print("Third")

everything = [first, second, third]
for func in everything:
    func()
#+end_src

#+RESULTS:
: First
: Second
: Third

However, we have to be able to call the functions in the same way in order for this trick to work, which means they must have the same signature.

Now suppose we have a function we want to test:

#+name: def sign
#+begin_src python
def sign(value):
    if value < 0:
        return -1
    else:
        return 1
#+end_src

and some functions that test it (two of which contain deliberate errors):

#+name: some-test-cases
#+begin_src python
def test_sign_negative():
    assert sign(-3) == -1

def test_sign_positive():
    assert sign(19) == 1

def test_sign_zero():
    assert sign(0) == 0

def test_sign_error():
    assert sgn(1) == 1
#+end_src

- Fixture :: The thing on which a test is run, such as the parameters to the function being tested or the file being processed.

- Each test does something to a *fixture* (such as the number 19) and uses assertions to compare the /actual/ result against the /expected/ result. The outcome of each test can be:

  * Pass :: the test subject works as expected.
  * Fail :: something is wrong with the test subject.
  * Error :: something is wrong in the test itself, which means we don’t know if the thing we’re testing is working properly or not.

- We can implement this classification scheme as follows:

  1. If a test function completes without /raising/ any kind of /exception/, it passes. (We don’t care if it returns something, but _by convention_ tests don’t return a value.)

  2. If the function raises an =AssertionError= exception, then the test has failed. Python’s ~assert~ statement does this automatically when the condition it is checking is false, so almost all tests use ~assert~ for checks.

  3. If the function raises any other kind of exception, then we assume the test itself is broken and count it as an error.

Translating these rules into code gives us the function 'run_tests' that runs every test in a list and counts how many outcomes of each kind it sees:

#+name: run_tests
#+begin_src python
def run_tests(all_tests):
    results = {"pass": 0, "fail": 0, "error": 0}
    for test in all_tests:
        try:
            test()
            results["pass"] += 1
        except AssertionError:
            results["fail"] += 1
        except Exception:
            results["error"] += 1
    print(f"pass {results['pass']}")
    print(f"fail {results['fail']}")
    print(f"error {results['error']}")
#+end_src

We use run_tests by putting all of our test functions into a list and passing that to the test runner:

#+begin_src python :noweb yes :eval yes
<<def sign>>
<<some-test-cases>>

<<run_tests>>

TESTS = [
    test_sign_negative,
    test_sign_positive,
    test_sign_zero,
    test_sign_error
]

run_tests(TESTS)

#+end_src

#+RESULTS:
: pass 2
: fail 1
: error 1

- NOTE :: *Independence*
  Our function runs tests in the order they appear in the list.

  The tests _SHOULD NOT_ rely on that: every unit test should work independently so that an error or failure in an early test doesn’t affect other tests’ behavior.

*** Finding functions

Making lists of functions is clumsy and error-prone: sooner or later we’ll add a function to TESTS twice or forget to add it at all.

We’d therefore like our test runner to find tests for itself, which it can do by exploiting the fact that Python stores variables in a structure similar to a dictionary.

#+caption: Using the 'globals' function
#+begin_src python :eval yes :session ch6-globals
import pprint
pprint.pprint(globals())
#+end_src

#+RESULTS:
#+begin_example
{'__PYTHON_EL_eval': <function __PYTHON_EL_eval at 0x780ae95c2ac0>,
 '__PYTHON_EL_eval_file': <function __PYTHON_EL_eval_file at 0x780ae95c2b60>,
 '__PYTHON_EL_native_completion_setup': <function __PYTHON_EL_native_completion_setup at 0x780ae95c2f20>,
 '__annotations__': {},
 '__builtins__': <module 'builtins' (built-in)>,
 '__doc__': None,
 '__loader__': <class '_frozen_importlib.BuiltinImporter'>,
 '__name__': '__main__',
 '__org_babel_python_format_value': <function __org_babel_python_format_value at 0x780ae95c2980>,
 '__package__': None,
 '__spec__': None,
 'pprint': <module 'pprint' from '/gnu/store/c5qarpvwzcz1idryvvpiqd7738jn4rs1-python-3.11.11/lib/python3.11/pprint.py'>,
 'readline': <module 'readline' from '/gnu/store/c5qarpvwzcz1idryvvpiqd7738jn4rs1-python-3.11.11/lib/python3.11/lib-dynload/readline.cpython-311-x86_64-linux-gnu.so'>,
 'tty': <module 'tty' from '/gnu/store/c5qarpvwzcz1idryvvpiqd7738jn4rs1-python-3.11.11/lib/python3.11/tty.py'>}
#+end_example

As the output shows, ~globals~ is a dictionary containing all the variables in the program’s /global scope/.

(See how the module ~pprint~ was imported)

Since we just started the interpreter, all we see are the variables that Python defines automatically.

*Q.* What happens when we define a variable of our own ?

#+begin_src python :eval yes :session ch6-globals
import pprint
my_variable = 123
def my_function(x,y):
    pass
pprint.pprint(globals())
#+end_src

#+RESULTS:
#+begin_example
{'__PYTHON_EL_eval': <function __PYTHON_EL_eval at 0x780ae95c2ac0>,
 '__PYTHON_EL_eval_file': <function __PYTHON_EL_eval_file at 0x780ae95c2b60>,
 '__PYTHON_EL_native_completion_setup': <function __PYTHON_EL_native_completion_setup at 0x780ae95c2f20>,
 '__annotations__': {},
 '__builtins__': <module 'builtins' (built-in)>,
 '__doc__': None,
 '__loader__': <class '_frozen_importlib.BuiltinImporter'>,
 '__name__': '__main__',
 '__org_babel_python_format_value': <function __org_babel_python_format_value at 0x780ae95c2980>,
 '__package__': None,
 '__spec__': None,
 'my_function': <function my_function at 0x780ae95c2e80>,
 'my_variable': 123,
 'pprint': <module 'pprint' from '/gnu/store/c5qarpvwzcz1idryvvpiqd7738jn4rs1-python-3.11.11/lib/python3.11/pprint.py'>,
 'readline': <module 'readline' from '/gnu/store/c5qarpvwzcz1idryvvpiqd7738jn4rs1-python-3.11.11/lib/python3.11/lib-dynload/readline.cpython-311-x86_64-linux-gnu.so'>,
 'tty': <module 'tty' from '/gnu/store/c5qarpvwzcz1idryvvpiqd7738jn4rs1-python-3.11.11/lib/python3.11/tty.py'>}
#+end_example

If function names are just variables and a program’s variables are stored in a dictionary, we can loop over that dictionary to find all the functions whose names start with 'test_':

#+name: def find-tests
#+begin_src python
def find_tests(prefix):
    for (name, func) in globals().items():
        if name.startswith(prefix):
            print(name, func)
#+end_src

#+begin_src python :noweb yes :eval yes
<<def find-tests>>
find_tests("test_")
#+end_src

#+RESULTS:
: test_sign_negative <function test_sign_negative at 0x7eefe1110900>
: test_sign_positive <function test_sign_positive at 0x7eefe1110cc0>
: test_sign_zero <function test_sign_zero at 0x7eefe1110d60>
: test_sign_error <function test_sign_error at 0x7eefe1110e00>

- The ~find_test~ function found the variable/functions in our python session (named 'ch6') whose name stated with 'test_'.

  The hexadecimal numbers in the output show where each function object is stored in memory.

  Having a running program find things in itself like this is called *introspection*, and is the key to many of the designs in upcoming chapters.

Combining introspection with the pass-fail-error pattern of the previous section gives us something that finds test functions, runs them, and summarizes their results:

#+name: def run_tests
#+begin_src python
def run_tests():
    results = {"pass": 0, "fail": 0, "error": 0}
    for (name, test) in globals().items():
        if not name.startswith("test_") or not callable(test):
            continue
        try:
            test()
            results["pass"] += 1
        except AssertionError:
            results["fail"] += 1
        except Exception:
            results["error"] += 1
    print(f"pass {results['pass']}")
    print(f"fail {results['fail']}")
    print(f"error {results['error']}")

#+end_src

In the above program we use ~callable~ to check if 'test' is actually a callable or not.

#+caption: Reason to use ~callable~
#+begin_src python
type(3)
# => <class 'int'>

def example():
    pass

type(example)
# => <class 'function'>

# Built-in functions have different type
type(len)
# => <class 'builtin_function_or_method'>

# Therefore, it is safert to use 'callable' to check if something can be called
callable(example)
# => True
callable(len)
# => True
#+end_src

** Questions
*** Looping over globals

*Q.* What happens if you run this code?
#+begin_src python :eval yes :session ch6-looping
for name in globals():
    print(name)
#+end_src

#+RESULTS:
: RuntimeError: dictionary changed size during iteration

*Q.* What happens if you run this code instead?
#+begin_src python :eval yes :session ch6-looping
name = None
for name in globals():
    print(name)
#+end_src

#+RESULTS:
#+begin_example
__name__
__doc__
__package__
__loader__
__spec__
__annotations__
__builtins__
tty
__PYTHON_EL_eval
__PYTHON_EL_eval_file
readline
__org_babel_python_format_value
__PYTHON_EL_native_completion_setup
name
#+end_example

*Q.* Why are the two different ?

*A.*

+ In the first example:

  When the =for= loop starts, =name= is not yet a variable in the global scope.

  As the loop iterates and assigns values to =name= (e.g., name = "__name__", name = "__doc__"), it implicitly /creates/ the =name= variable in the global scope.

  Creating a new global variable adds an entry to the ~globals()~ dictionary.
  Python disallows modifying a dictionary (like ~globals()~) while it is being iterated over, leading to a =RuntimeError=.

  - NOTE :: We get the same error EVEN IF we had used ~globals().items()~

+ In the second example:
  The line ~name = None~ /before/ the loop explicitly creates the =name= variable in the global scope.

  When the =for= loop begins, =name= already exists in the =globals()= dictionary. The loop then simply /reassigns/ the value of the /existing/ =name= variable in each iteration.

  This is not a modification of the dictionary's structure (no new key is added or removed during iteration), so no =RuntimeError= occurs.

*** Local Variables
Python has a function called ~locals~ that returns all the variables defined in the current /local scope/.
(local -- Referring to the current or innermost scope in a program.)



1. Predict what the code below will print before running it. When does the variable i first appear and is it still there in the final line of output?

2. Run the code and compare your prediction with its behavior.

#+begin_src python :eval yes :session ch6-locals
def show_locals(low, high):
    print(f"start: {locals()}")
    for i in range(low, high):
        print(f"loop {i}: {locals()}")
    print(f"end: {locals()}")

show_locals(1, 3)
#+end_src

#+RESULTS:
: start: {'low': 1, 'high': 3}
: loop 1: {'low': 1, 'high': 3, 'i': 1}
: loop 2: {'low': 1, 'high': 3, 'i': 2}
: end: {'low': 1, 'high': 3, 'i': 2}

The point here is that =i= still remains ever after the end of the for-loop.
This is because (I think) in python the innermost scope is the function scope, i.e. the for-loop doesn't have its own scope.

* Chapter 7: Running Tests
:PROPERTIES:
:ref: https://third-bit.com/sdxpy/interp/
:header-args:python: :session ch7 :results output :async yes :eval no
:header-args:sh: :session ch7-sh :results output :async yes
:END:

** Summary
:PROPERTIES:
:ID:       0a6eb5bf-e600-4902-9b35-011cc152a755
:END:
- Compilers and interpreters are just programs.
- Basic arithmetic operations are just functions that have special notation.
- Programs can be represented as trees, which can be stored as nested lists.

- Interpreters recursively _dispatch_ operations to functions that implement low-level steps.

- Programs store variables in /stacked/ dictionaries called *environments*.

- One way to evaluate a program's design is to ask how extensible it is.

#+caption: Interpreter concept map
[[file:images/Chapter_7:_Running_Tests/2025-08-25_10-57-23_screenshot.png]]

The central idea is that a program is just another kind of data.
Please see Appendix B for extra material related to these ideas.

** Theory
*** Intro

Most real programming languages have two parts:
1. a parser that translates the source code into a data structure,
2. and a runtime that executes the instructions in that data structure.

#+caption: Two ways to run Code
#+begin_quote
A compiler translates a program into runnable instructions before the program runs, while an interpreter generates instructions /on the fly/ as the program is running.

The differences between the two are BLURRY in practice:

for example, Python translates the instructions in a program into instructions as it loads files, but saves those instructions in .pyc files to save itself work the next time it runs the program.
#+end_quote

*** Expressions
:PROPERTIES:
:ID:       ee521d45-e4ef-4b47-8ead-b35cae5944e9
:END:

Let’s start by building something that can evaluate simple *expressions* such as 1+2 or abs(-3.5).

We represent each expression as a list with the name of the operation as the first item and the values to be operated on as the other items. If we have multiple operations, we use nested lists:

#+begin_src python
["add", 1, 2]            # 1 + 2
["abs", -3.5]            # abs(-3.5)
["add", ["abs", -5], 9]  # abs(-5) + 9
#+end_src

#+name: def do_add
#+begin_src python
def do_add(args):
    assert len(args) == 2
    left = do(args[0])
    right = do(args[1])
    return left + right
#+end_src

#+name: def do_abs
#+begin_src python
def do_abs(args):
    assert len(args) == 1
    val = do(args[0])
    return abs(val)
#+end_src

- NOTE :: that ~do_abs~ and ~do_add~ have the same signature. As with the unit testing functions in [[*Chapter 6: Running Tests][Chapter 6: Running Tests]], this allows us to call them interchangeably.

*Q.* So how does ~do~ work ?

*A.* It starts by checking if its input is an integer. If so, it returns that value right away because integers “evaluate” to themselves. Otherwise, ~do~ checks that its parameter is a list and then uses the first value in the list to decide what other function to call.

#+name: do
#+begin_src python
def do(expr):
    # Integers evaluate to themselves.
    if isinstance(expr, int):
        return expr

    # Lists trigger function calls.
    assert isinstance(expr, list)
    if expr[0] == "abs":
        return do_abs(expr[1:])
    if expr[0] == "add":
        return do_add(expr[1:])
    assert False, f"Unknown operation {expr[0]}"

#+end_src

This lookup-and-call process is called *dynamic dispatch*, since the program decides who to give work to on the fly.

It leads to a situation where ~do~ calls a function like ~do_add~, which in turn calls ~do~, which may then call ~do_add~ (or something other function) and so on.

- Dynamic dispatch ::
  To find a function or a property of an /object/ by name while a program is running.

  For example, instead of getting a specific property of an object using ~obj.name~, a program might use ~obj[someVariable]~, where someVariable could hold "name" or some other property name.

With all of this code in place, the main body of the program can read the file containing the instructions to execute, call do, and print the result:

#+begin_src python :tangle ./src/ch7/main.py :noweb yes
import sys
import json

<<def do_add>>

<<def do_abs>>

<<do>>

def main():
    assert len(sys.argv) == 2, "Usage: expr.py filename"
    with open(sys.argv[1], "r") as reader:
        program = json.load(reader)
    result = do(program)
    print(f"=> {result}")

if __name__ == "__main__":
    main()
#+end_src

#+begin_src python :tangle ./src/ch7/expr.tll
["add", ["abs", -3], 2]
#+end_src

#+begin_src sh
uv run ./src/ch7/main.py ./src/ch7/expr.tll
#+end_src

#+RESULTS:
: => 5

- Python reads expr.py, turns it into a data structure with operation identifiers and constants, and uses those operation identifiers to decide what functions to call.

  The functions inside Python are written in C and have been compiled to machine instructions, but the cycle of lookup and call _IS EXACTLY THE SAME_ as it is in our little interpreter.

*** Variables

Doing arithmetic on constants is a start, but our programs will be easier to read if we can define variables that give names to values.

We can add variables to our interpreter by passing around a dictionary containing all the variables seen so far.

Such a dictionary is sometimes called an *environment* because it is the setting in which expressions are evaluated; the dictionaries returned by the ~globals()~ and ~locals()~ functions introduced in Chapter 6 are both environments.

Let’s modify do_add, do_abs, do, and main to take an environment as an extra parameter and pass it on as needed:

#+name: def do; env, expr
#+begin_src python
def do(env, expr):
    # Integers evaluate to themselves.
    if isinstance(expr, int):
        return expr

    # Lists trigger function calls.
    assert isinstance(expr, list)
    if expr[0] == "abs":
        return do_abs(env, expr[1:])
    if expr[0] == "add":
        return do_add(env, expr[1:])
    if expr[0] == "get":
        return do_get(env, expr[1:])
    if expr[0] == "seq":
        return do_seq(env, expr[1:])
    if expr[0] == "set":
        return do_set(env, expr[1:])
    assert False, f"Unknown operation {expr[0]}"
#+end_src

#+name: do this and that
#+begin_src python
def do_add(env, args):
    assert len(args) == 2
    left = do(env, args[0])
    right = do(env, args[1])
    return left + right

def do_abs(env, args):
    assert len(args) == 1
    val = do(env, args[0])
    return abs(val)

# Looking up variables when we need their values is straightforward. We check
# that we have a variable name and that the name is in the environment, then
# return the stored value:
def do_get(env, args):
    assert len(args) == 1
    assert isinstance(args[0], str)
    assert args[0] in env, f"Unknown variable {args[0]}"
    return env[args[0]]

# To define a new variable or change an existing one, we evaluate an expression
# and store its value in the environment:
def do_set(env, args):
    assert len(args) == 2
    assert isinstance(args[0], str)
    value = do(env, args[1])
    env[args[0]] = value
    return value

# We need to add one more function to make this all work. Our programs no longer
# consist of a single expression; instead, we may have several expressions that
# set variables’ values and then use them in calculations. To handle this, we
# add a function do_seq that runs a sequence of expressions one by one. This
# function is our first piece of control flow: rather than calculating a value
# itself, it controls when and how other expressions are evaluated. Its
# implementation is:
def do_seq(env, args):
    assert len(args) > 0
    for item in args:
        result = do(env, item)
    return result

#+end_src

#+begin_src python :tangle ./src/ch7/main2.py :noweb yes
import sys
import json

<<do this and that>>

<<def do; env, expr>>

def main():
    assert len(sys.argv) == 2, "Usage: expr.py filename"
    with open(sys.argv[1], "r") as reader:
        program = json.load(reader)
    env = {} # initialize the environment
    result = do(env, program)
    print(f"=> {result}")

if __name__ == "__main__":
    main()

#+end_src

#+begin_src python :tangle ./src/ch7/expr2.tll
[
    "seq",
    ["set", "alpha", 1],
    ["set", "beta", 2],
    ["add", ["get", "alpha"], ["get", "beta"]]
]
#+end_src

#+begin_src sh
uv run ./src/ch7/main2.py ./src/ch7/expr2.tll
#+end_src

#+RESULTS:
: => 3

#+caption: Everything is an expression
[[file:images/Chapter_7:_Running_Tests/2025-08-25_11-29-15_screenshot.png]]

*** Introspection
:PROPERTIES:
:ID:       122a8a24-1485-43e1-9e20-ce9512e9f212
:END:

In [[def do; env, expr]], the sequence of =if= statements that decide what function to call is becoming unwieldy.

We can replace this by using *introspection* to create a lookup table that stores every function whose name starts with 'do_'

- Introspection/Reflection ::
  To inspect the properties of a running program in a generic way.
  Reflection relies on the fact that a program is just another data structure.

  #+caption: Dynamically-generated function lookup table
  [[file:images/Chapter_7:_Running_Tests/2025-08-25_13-32-29_screenshot.png]]

We wanna map functions like 'do_operationName' to the name 'operationName' so that we can get rid of our =if= statements.

#+begin_src python :tangle ./src/ch7/main3.py :noweb yes
import sys
import json

<<do this and that>>

OPS = {
    name.replace('do_', ''): func
    for name, func in globals().items()
    if name.startswith('do_') and callable(func)
}

def do(env, expr):
    # Integers evaluate to themselves.
    if isinstance(expr, int):
        return expr

     # Lists trigger function calls.
    assert isinstance(expr, list)
    assert expr[0] in OPS, f"Unknown operation {expr[0]}"
    func = OPS[expr[0]]
    return func(env, expr[1:])

def main():
    assert len(sys.argv) == 2, "Usage: expr.py filename"
    with open(sys.argv[1], "r") as reader:
        program = json.load(reader)
    env = {} # initialize the environment
    result = do(env, program)
    print(f"=> {result}")

if __name__ == "__main__":
    main()
#+end_src

As with unit test functions in [[*Chapter 6: Running Tests][Chapter 6: Running Tests]] , the ~do_*~ functions MUST HAVE exactly the same signature so that we can call any of them with an environment and a list of arguments _without knowing exactly which function we’re calling_.

And as with finding tests, introspection is more reliable than a hand-written lookup table BUT is harder to understand.

#+begin_src sh
uv run ./src/ch7/main3.py ./src/ch7/expr2.tll
#+end_src

#+RESULTS:
: => 3

* TODO Chapter 8: Functions and Closures
:PROPERTIES:
:ref: https://third-bit.com/sdxpy/func/
:header-args:python: :session ch8 :results output :async yes :eval no
:header-args:sh: :session ch8-sh :results output :async yes
:END:
