#+title: Software Design by Example
#+subtitle: A Tool-Based Introduction with Python
#+auto_tangle: t
#+startup: overview
https://third-bit.com/sdxpy/

* Chapter 2: Objects and Classes
:PROPERTIES:
:ref: https://third-bit.com/sdxpy/oop/
:header-args:python: :session ch2 :results output :async yes
:END:
** Theory
*** Intro

#+CAPTION: Python class example (with inheritance)
#+begin_src python
import math

class Shape:
    def __init__(self, name):
        self.name = name

    def perimeter(self):
        raise NotImplementedError

    def area(self):
        raise NotImplementedError

    def density(self, weight):
        return weight / self.area()

class Square(Shape):
    def __init__(self, name, side):
        super().__init__(name)
        self.side = side

    def perimeter(self):
        return 4 * self.side

    def area(self):
        return self.side ** 2

class Circle(Shape):
    def __init__(self, name, radius):
        super().__init__(name)
        self.radius = radius

    def perimeter(self):
        return 2 * math.pi * self.radius

    def area(self):
        return math.pi * (self.radius ** 2)

#+end_src

#+RESULTS:

<<making constructors>>
- NOTE :: How in the subclasses' constructor we have to *manually*
  1. mention the arguments (to ~__init__~) and
  2. call the ~super~ method

Since =Square= and =Circle= have the same methods, we can use them interchangibly.
This is called *polymorphism*.
It reduces cognitive load by allowing the people using related things to ignore their differences:

#+begin_src python
examples = [Square("sq", 3), Circle("ci", 2)]
for thing in examples:
    n = thing.name
    p = thing.perimeter()
    a = thing.area()
    d = thing.density(5)
    print(f"{n} has perimeter {p:.2f}, area {a:.2f} and density {d:.2f}")
#+end_src

#+RESULTS:
: sq has perimeter 12.00, area 9.00 and density 0.56
: ci has perimeter 12.57, area 12.57 and density 0.40

*** Using dictionary to implement polymorphism
:PROPERTIES:
:ID:       c5c68489-11e0-4960-bb9b-f003d127ac63
:header-args:python: :session ch2 :results output :eval no :async yes
:END:

#+CAPTION: Concept map for implementing objects and classes
[[file:images/Chapter_2:_Objects_and_Classes/2025-08-17_15-49-47_screenshot.png]]

#+NAME: my-classes
#+begin_src python :noweb yes :eval yes
from pprint import pprint

<<my-class-methods>>
<<my-class-constructors>>
<<generic-make-method>>

MyShape = {
    # methods
    "density": shape_density,# this method will be inherited
    # metadata
    "_classname": "MyShape", # obj._classname will tell the class of an object
    "_new": make_my_shape,
    "_parent": None, # obj._parent will is parent class of this object's class
}

MySquare = {
    "perimeter": square_perimeter,
    "area": square_area,
    # metadata
    "_new": make_my_square,
    "_classname": "MySquare",
    "_parent": MyShape,
}

MyCircle = {
    "perimeter": circle_perimeter,
    "area": circle_area,
    # metadata
    "_new": make_my_circle,
    "_classname": "MyCircle",
    "_parent": MyShape,
}

pprint(make(MyCircle, 'cute circle', radius=2))
pprint(make(MySquare, 'stupid square', side=4))
#+end_src

#+RESULTS: my-classes
#+begin_example
{'class': {'_classname': 'MyCircle',
           '_new': <function make_my_circle at 0x7ed8a19349a0>,
           '_parent': {'_classname': 'MyShape',
                       '_new': <function make_my_shape at 0x7ed8a19345e0>,
                       '_parent': None,
                       'density': <function shape_density at 0x7ed8a19351c0>},
           'area': <function circle_area at 0x7ed8a1934fe0>,
           'perimeter': <function circle_perimeter at 0x7ed8a1934ae0>},
 'name': 'cute circle',
 'radius': 2}
{'class': {'_classname': 'MySquare',
           '_new': <function make_my_square at 0x7ed8a19360c0>,
           '_parent': {'_classname': 'MyShape',
                       '_new': <function make_my_shape at 0x7ed8a19345e0>,
                       '_parent': None,
                       'density': <function shape_density at 0x7ed8a19351c0>},
           'area': <function square_area at 0x7ed8a1937e20>,
           'perimeter': <function square_perimeter at 0x7ed8a1937240>},
 'name': 'stupid square',
 'side': 4}
#+end_example

Our classes only define:
+ ~_classname~ :: the class' name
+ ~_new~ :: their constructor
  - NOTE :: requires the Class, which in turn requires this constructor, but it works :)
+ ~_parent~ :: reference to their parent class.
  Therefore, we also /automatically/ inherit the method's of the parent class.
+ the class' methods

#+CAPTION: Constructors for MySquare and MyCircle
#+NAME: my-class-constructors
#+begin_src python :noweb yes
def make(cls, *args, **kwargs):
    """
    Generic make function
    """
    return cls['_new'](*args, **kwargs)

def make_my_shape(name):
    return {
        "name": name,
        "class": MyShape,
    }

def make_my_square(name, side):
    # calling the super method
    # assigning new attributes
    #
    # MySquare overwrites the 'class' key
    # (we use '|' to combine 2 dicts)
    return make(MyShape, name) | {
        "side": side,
        "class": MySquare,
    }

def make_my_circle(name, radius):
    return make(MyShape, name) | {
        "radius": radius,
        "class": MyCircle,
    }
#+end_src

Each class constructor only defines:
+ ~class~ :: reference to its class
+ its attributes (_all manual_)
  See [[making constructors]]
  - NOTE :: Here we are mentioning all the attributes, even those of the parent class.
    - This is a *manual* step.
      When making the constructor for some class, we need to /THINK/ about its parent class and the arguments of that class' constructor.
      (Also, Python can only have one ~__init__~  method)

#+CAPTION: Methods for MyShape, MySquare, MyCircle
#+NAME: my-class-methods
#+begin_src python
# In the below function the 'self' represents the obj
def shape_density(self, weight):
    return weight / call_method(self, "area")

def square_perimeter(self):
    return 4 * self['side']

def square_area(self):
    return self['side'] ** 2

def circle_perimeter(self):
    return 2 * math.pi * self['radius']

def circle_area(self):
    return math.pi * (self['radius'] ** 2)
#+end_src

- Notice how some 'Shape' object would call its 'density' method:
  ~obj["density"](obj, weight)~

  which is very similar to:
  ~obj.density(weight)~ (here the 'self' object passed is implicit)

  - Also, not the definition of the 'shape_density' function.
    Its arguments are the same as that of a normal class' methods.

*** call_method implementation

The call_method implementation is important as that is what ties all this together.

It has to look like ~call_method(object, method_name, method_arguments ...)~

#+NAME: call_method
#+begin_src python
def call_method(obj, method_name: str, *method_args, **method_kwargs):
    def find_method(obj, method_name):
        "Returns the method or None (if unable to find)"
        cls = obj['class']
        while cls:
            if method_name in cls:
                return cls[method_name]
            cls = cls['_parent']
        return None

    method = find_method(obj, method_name)
    print(f'calling method: {method.__name__}, with args: {method_args}, with kwargs: {method_kwargs}')
    return method(obj, *method_args, **method_kwargs)
#+end_src

#+RESULTS: call_method

#+begin_src python
examples = [make(MySquare, "sq", 3), make(MyCircle, "ci", 2)]
for ex in examples:
    n = ex["name"]
    d = call_method(ex, "density", 5)
    print(f"{n}: {d:.2f}")
    print()
#+end_src

#+RESULTS:
: calling method: shape_density, with args: (5,), with kwargs: {}
: calling method: square_area, with args: (), with kwargs: {}
: sq: 0.56
:
: calling method: shape_density, with args: (5,), with kwargs: {}
: calling method: circle_area, with args: (), with kwargs: {}
: ci: 0.40

** Questions
*** Class methods and static methods

*Q.* Explain the differences between class methods and static methods
*A.* [[https://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python][StackOverflow answer]]

*Q.* Implement both using dictionaries.

We need changes:
- in the ~call_method~ function
- how we store a method
  - we have to mention the function implementation as well as the type of method it is, one of:
    - normal method ~def (self, ...)~
    - classmethod ~def (cls, ...)~
    - staticmethod ~def (...)~ (No self or cls)

* Chapter 3: Finding duplicate files
:PROPERTIES:
:ref: https://third-bit.com/sdxpy/dup/
:header-args:python: :session ch3 :results output :async yes
:GPTEL_TOPIC: chapter-3:-finding-duplicate-files
:END:
** Summary
:PROPERTIES:
:ID:       ba463b34-d27b-4adc-a0ee-9021cd241ad9
:END:

- A hash function:
  * creates a fixed-size value from an arbitrary sequence of bytes.
  * has deterministic output, but it is not easy to predict.
  * if good, has evenly distributed output.
- A large cryptographic hash can be used to uniquely identify a file's contents.

#+caption: summarizes the key ideas in this chapter, the most important of which is that SOME ALGORITHMS ARE INTRINSICALLY BETTER THAN OTHERS.
[[file:images/Chapter_3:_Finding_duplicate_files/2025-08-18_15-14-09_screenshot.png]]

** Theory
*** Setup

#+caption: Prepare files
#+begin_src sh
mkdir -p ./src/ch3/files

$(
    cd ./src/ch3/files

    echo aaa > a1.txt
    echo aaa > a2.txt
    echo aaa > a3.txt
    echo bb > b1.txt
    echo bb > b2.txt
    echo c > c1.txt
)

ls ./src/ch3/files
#+end_src

#+RESULTS:
| a1.txt |
| a2.txt |
| a3.txt |
| b1.txt |
| b2.txt |
| c1.txt |

#+name: filenames
#+begin_src python
from pprint import pprint

filenames = [
    './src/ch3/files/a1.txt',
    './src/ch3/files/a2.txt',
    './src/ch3/files/a3.txt',
    './src/ch3/files/b1.txt',
    './src/ch3/files/b2.txt',
    './src/ch3/files/c1.txt'
]
#+end_src

*** Naive approach
If we have /N/ files.
To find all duplicate files, it will take us N-1 + N-2 + ... + 1 = O(N^2) comparisions.
Each of the comparision also involves expensive byte-by-byte comparision on files the _might_ be equal.

#+name: same-bytes
#+begin_src python
def same_bytes(file1: str, file2: str, log=False) -> bool:
    bytes1 = open(file1, 'rb').read()
    bytes2 = open(file2, 'rb').read()
    if log:
        print(bytes1)
        print(bytes2)
    return bytes1 == bytes2
#+end_src

#+begin_src python :noweb yes
<<same-bytes>>
print(same_bytes('./src/ch3/files/a1.txt', './src/ch3/files/c1.txt', log=True))
#+end_src

#+RESULTS:
: b'aaa\n'
: b'c\n'
: False

- Note ::  that the files are opened in binary mode using ~"rb"~ instead of the
  usual ~"r"~.
  - This tells Python to read the bytes exactly as they are rather than trying to convert them to characters.

#+name: naive
#+caption: O(n^2) algorithm
#+begin_src python :noweb yes
<<filenames>>
<<same-bytes>>

def find_duplicates(filenames):
    duplicates = []
    n = len(filenames)
    for i in range(n):
        file1 = filenames[i]
        for j in range(i+1, n):
            file2 = filenames[j]
            if same_bytes(file1, file2):
                duplicates.append((file1, file2))
    return duplicates

pprint(find_duplicates(filenames))
#+end_src

#+RESULTS: naive
: [('./src/ch3/files/a1.txt', './src/ch3/files/a2.txt'),
:  ('./src/ch3/files/a1.txt', './src/ch3/files/a3.txt'),
:  ('./src/ch3/files/a2.txt', './src/ch3/files/a3.txt'),
:  ('./src/ch3/files/b1.txt', './src/ch3/files/b2.txt')]

*** Hash approach
Instead of comparing every file against every other, let’s process each file once to produce a short identifier that depends only on the file’s contents and then only compare files that have the same identifier, i.e., that /might/ be equal.

If files are evenly divided into =g= groups then each group will contain roughly =N/g= files, so the total work will be roughly =g*(N/g)^2=.

Simplifying, this is =N^2/g= , so as the number of groups grows, and the overall running time should decrease.

- We can use *hashing* to split the files into groups.
  We construct IDs for files by using a /hash function/ to produce a /hash code/.

  Since bytes are just numbers, we can create a very simple hash function by adding up the bytes in a file and taking the remainder module some number (the no. of groups we want).

#+caption: A naive hash function
#+begin_src python
def naive_hash(data):
    return sum(data) % 13

example = bytes('file content', 'utf-8')
print(f'Example: {example}')

id = naive_hash(example)
print(f'id: {id} \n')

for byt in example:
    print(f'byte: {byt}')
print(f'\nsum: {sum(example)}, sum%13 = {sum(example)%13}')
#+end_src

#+RESULTS:
#+begin_example
Example: b'file content'
id: 2

byte: 102
byte: 105
byte: 108
byte: 101
byte: 32
byte: 99
byte: 111
byte: 110
byte: 116
byte: 101
byte: 110
byte: 116

sum: 1211, sum%13 = 2
#+end_example

*** Better hashing
For =g= groups we have to perform =O(N^2/g)= comparisions.
If /g = N/, i.e. for each file its id has _a unique bucket_, then our complexiy becomes =O(N)=.

- NOTE :: We have to read each file at least once anyway, so we can’t possibly do better than =O(N)=.

*Q.* How can we ensure that each unique file winds up in its own group?

*A.* Use a /cryptographic hash function/:

   - The output of such a function is completely deterministic: given the same bytes in the same order, it will always produce the same output.

   - However, the output is distributed like a uniform random variable: each possible output is equally likely, which ensures that files will be evenly distributed between groups.

- SHA256 hashing algorithm :: Given some bytes as input, this function produces a 256-bit hash, which is normally written as a 64-character (as 64x4=256) hexadecimal (4bit) string.

#+name: sha256
#+begin_src python
from hashlib import sha256

# strings must be encoded before hashing
example = bytes('some content', 'utf-8')

output = sha256(example).hexdigest()
print(output)
#+end_src

#+RESULTS: sha256
: 290f493c44f5d63d06b374d0a5abd292fae38b92cab2fae5efefe1b0e9347f56

#+name: duplicates
#+caption: O(n) algorithm
#+begin_src python :noweb yes
from hashlib import sha256
<<filenames>>

def hash_files(filenames):
    groups = dict()
    for filename in filenames:
        data = open(filename, 'rb').read()
        hash_code = sha256(data).hexdigest()
        if hash_code not in groups:
            groups[hash_code] = set()
        groups[hash_code].add(filename)
    return groups

for files in hash_files(filenames).values():
    # each set of files is a duplicate of each other
    print(files)
#+end_src

#+RESULTS: duplicates
: {'./src/ch3/files/a2.txt', './src/ch3/files/a1.txt', './src/ch3/files/a3.txt'}
: {'./src/ch3/files/b2.txt', './src/ch3/files/b1.txt'}
: {'./src/ch3/files/c1.txt'}

*** Birthday problem

The odds that two people share a birthday are 1/365 (ignoring February 29).

The odds that they don’t are therefore \(\frac{365}{365}\)x\(\frac{364}{365}\).
When we add a third person, the odds that nobody share a birthday are \(\frac{365}{365}\)x\(\frac{364}{365}\)x\(\frac{363}{365}\).

If we keep going, there’s a 50% chance of two people sharing a birthday in a group of just 23 people, and a _99.9% chance with 70 people_.

The same math can tell us how many files we need to hash before there’s a 50% chance of a collision with a 256-bit hash. According to Wikipedia, the answer is approximately 4 x 10^38 files.
We’re willing to take that risk.

** Questions
*** Odds of collision
If hashes were only 2 bits long, then the chances of collision with each successive file assuming no previous collision are:

| Number of Files | Odds of Collision |
|-----------------+-------------------|
|               1 |                0% |
|               2 |               25% |
|               3 |               50% |
|               4 |               75% |
|               5 |              100% |

*Q.* A colleague of yours says this means that if we hash four files, there’s only a 75% chance of any collision occurring. What are the actual odds?

*A.* This is _a common confusion_.

2 bits means 4 possible codes.
The odds of collision:
- #files = 1 is 0.
- #files = 2 is 1/4 (second file will collide if it takes the value of the first file)
- #files = 3 is 2/4 (third file will collide if it takes value of either of the previous files)
- #files = 4 is 3/4
- #files >= 5 is 1.

But, if we hash 4 files the odds of them not colliding is = 4/4 x 3/4 x 2/4 x 1/4 = 3/32
Therefore, the odds of their being 1 collision when hashing 4 files = 1 - 3/32 = 0.90625 i.e. 90.625%

*** Streaming

- streaming API ::
  An API that processes data in chunks rather than needing to have all of it in memory at once.
  Streaming APIs usually require handlers for events such as:
  - “start of data”,
  - “next block”, and
  - “end of data”

A streaming API delivers data one piece at a time rather than all at once. Read the documentation for the update method of hashing objects in Python’s [[https://docs.python.org/3/library/hashlib.html][hashing module]] and rewrite the duplicate finder from this chapter to use it.

#+caption: streaming hashing
#+begin_src python :noweb yes
from hashlib import sha256

m = sha256()
# => m = <sha256 _hashlib.HASH object @ 0x71eb1a40c730>

m.update(b"Nobody inspects")
m.update(b" the spammish repetition")

m.digest()
# => b'\x03\x1e\xdd}Ae\x15\x93\xc5\xfe\\\x00o\xa5u+7\xfd\xdf\xf7\xbcN\x84:\xa6\xaf\x0c\x95\x0fK\x94\x06'

m.hexdigest()
# => '031edd7d41651593c5fe5c006fa5752b37fddff7bc4e843aa6af0c950f4b9406'

sha256(b"Nobody inspects the spammish repetition").hexdigest()
# => '031edd7d41651593c5fe5c006fa5752b37fddff7bc4e843aa6af0c950f4b9406'
#+end_src

* Chapter 4: Matching patterns
:PROPERTIES:
:ref: https://third-bit.com/sdxpy/glob/
:header-args:python: :session ch4 :results output :async yes
:header-args:sh: :session ch4-sh :results output :async yes
:END:
** Summary
:PROPERTIES:
:ID:       1e611433-31e2-4a18-800b-903d2eebc169
:END:

- Use globs and regular expressions to match patterns in text.
- Use inheritance to make matchers composable and extensible.
- Simplify code by having objects delegate work to other objects.

- Use the *Null Object pattern* to eliminate special cases in code.

- Use standard refactorings to move code from one working state to another.
- Build and check the parts of your code you are least sure of first to find out if your design will work.

#+caption: summarizes the key ideas in this chapter; we will see the *Null Object* and *Chain of Responsibility* design patterns again.
[[file:images/Chapter_4:_Matching_patterns/2025-08-18_16-54-52_screenshot.png]]

** Theory
*** Setup

#+begin_src sh
mkdir -p ./src/ch4/
#+end_src

*** Simple patterns
:PROPERTIES:
:ID:       f9f26345-73e4-4da9-a36c-0fbb127cf219
:END:

|---------+--------+--------|
| *Pattern* | *Text*   | *Match?* |
|---------+--------+--------|
| abc     | “abc”  | true   |
| ab      | “abc”  | false  |
| abc     | “ab”   | false  |
| *       | ”“     | true   |
| *       | “abc”  | true   |
| a*c     | “abc”  | true   |
| {a,b}   | “a”    | true   |
| {a,b}   | “c”    | false  |
| {a,b}   | “ab”   | false  |
| *{x,y}  | “abcx” | true   |
|---------+--------+--------|

+ Matching is conceptually simple.

  * If the first element of the pattern matches the target string at the current location, we check if the rest of the pattern matches what’s left of the string.

  * If the element doesn’t match the front of the string, or if the rest of the pattern can’t match the rest of the string, matching fails. (This behavior makes globbing different from regular expressions, which can match parts of strings.)

This design makes use of the *Chain of Responsibility* design pattern.
Each matcher matches if it can then asks the next matcher in the chain to try to match the remaining text (Figure 4.2).
Crucially, objects don’t know how long the chain after them is: they just know whom to ask next.

#+caption: matching with Chain of Responsibility
[[file:images/Chapter_4:_Matching_patterns/2025-08-18_17-34-00_screenshot.png]]

- Chain of Responsibility pattern ::
  A design pattern in which each object either handles a request or passes it on to another object.

- Null Object pattern ::
  A design pattern in which a placeholder object is used instead of /None/.

  The placeholder object has the methods of the object usually used, but those methods do 'nothing'.
  - NOTE :: 'nothing' DOESN'T mean that the return value of those methods has to be /None/.

  This pattern saves other code from having to check repeatedly for /None/.

So, we will define a base class ~Pattern~ that will have a method ~match~ which returns boolean indicating if the ~Pattern~ matches some text.
To refer to the next ~Pattern~ we will note that down in its 'rest' attribute.

#+name: base-class
#+caption: base and null object class
#+begin_src python :tangle ./src/ch4/matcher.py :noweb yes
Pattern = type('Pattern')
class Pattern:
    def __init__(self, rest: Pattern | None = None):
        """
        Subclasses of Pattern can take two arguments:
        1. set of characters, a str etc.
        2. rest (the next Pattern)
        """
        # NOTE: How we can pass the argument has None and it is handled by the
        # Null Object pattern
        self.rest : Pattern = rest if rest is not None else Null()

    def match(self, text: str) -> bool:
        """
        Returns boolean indicating if Pattern matches `text`.
        """
        length_of_matched_text = self._match(text, start=0)
        return len(text) == length_of_matched_text

    <<Pattern __eq__>>

class Null(Pattern):
    """
    Null Object Pattern
    Null() is the placeholder object instead of 'None'
    """
    def __init__(self):
        """
        Null objects must be at the end of the matching chain, i.e., their 'rest'
        must be None, so we remove the 'rest' parameter from the class’s
        constructor and pass 'None' up to the parent constructor every time.
        """
        self.rest = None # base case

    def _match(self, text, start):
        """
        Since Null objects don’t match anything, Null._match immediately returns
        whatever starting point it was given.

        Every other matcher can now pass responsibility down the chain without
        having to test whether it’s the last matcher in line or not.
        """
        return start

<<literal pattern>>
<<any pattern>>
<<either pattern>>
#+end_src

- ~Match.rest~ requires every child class to have a helper method called ~_match~ that returns the location from which searching is to continue.

- ~Match.match~ checks whether the entire match reaches the end of the target string and returns True or False as appropriate.

*** Implement Lit(Pattern)

#+caption: literal tests
#+begin_src python :tangle ./src/ch4/test_literal.py
from matcher import *

def test_literal_match_entire_string():
    # /abc/ matches "abc"
    assert Lit("abc").match("abc")

def test_literal_substring_alone_no_match():
    # /ab/ doesn't match "abc"
    assert not Lit("ab").match("abc")

def test_literal_superstring_no_match():
    # /abc/ doesn't match "ab"
    assert not Lit("abc").match("ab")
#+end_src

For the above tests we define the following ~Literal~ pattern:

#+name: literal pattern
#+begin_src python :noweb yes
class Lit(Pattern):
    def __init__(self, chars: str, rest=None):
        super().__init__(rest)
        self.chars = chars

    def _match(self, text: str, start=0):
        end = len(self.chars) + start
        if text[start:end] != self.chars:
            # failed
            #
            # this is the position to next search,
            # (therefore None means failed)
            return None
        # passed
        return self.rest._match(text, start=end)

    <<Lit __eq__>>
#+end_src


#+caption: literal test to make sure chaining is working
#+begin_src python :tangle ./src/ch4/test_literal.py
def test_literal_followed_by_literal_match():
    # /a/+/b/ matches "ab"
    assert Lit("a", Lit("b")).match("ab")

def test_literal_followed_by_literal_no_match():
    # /a/+/b/ doesn't match "ac"
    assert not Lit("a", Lit("b")).match("ac")
#+end_src


#+begin_src sh :async yes
uvx pytest ./src/ch4/test_literal.py
#+end_src

#+RESULTS:
: =============================== test session starts ================================
: platform linux -- Python 3.12.8, pytest-8.4.1, pluggy-1.6.0
: rootdir: /home/nabeel/Documents/public/books/Software Design by Example
: collected 5 items
:
: src/ch4/test_literal.py .....                                                [100%]
:
: ================================ 5 passed in 0.01s =================================

*** Implement Any(Pattern)

#+caption: any's tests
#+begin_src python :tangle ./src/ch4/test_any.py
from matcher import *

def test_any_matches_empty():
    # /*/ matches ""
    assert Any().match("")

def test_any_matches_entire_string():
    # /*/ matches "abc"
    assert Any().match("abc")

def test_any_matches_as_prefix():
    # /*def/ matches "abcdef"
    assert Any(Lit("def")).match("abcdef")

def test_any_matches_as_suffix():
    # /abc*/ matches "abcdef"
    assert Lit("abc", Any()).match("abcdef")

def test_any_matches_interior():
    # /a*c/ matches "abc"
    assert Lit("a", Any(Lit("c"))).match("abc")
#+end_src

Keeping in mind the above tests, we write the following class:

#+name: any pattern
#+begin_src python
class Any(Pattern):
    def __init__(self, rest=None):
        super().__init__(rest)

    def _match(self, text: str, start=0):
        """
        Here '*' can match 0 or more of the characters
        in text[start:].
        Here, we implement lazy matching, '*' will match
        the shortes string so that the 'rest' of Pattern can
        successfully match 'text'.
        """
        n = len(text)
        # length matched = from 0 -> len(text[start:])
        #
        # NOTE1: i=n+1 => * matched the complete text[start:]
        #       and the 'rest' will have to match the empty string.
        for i in range(start, n+1):
            # i the index where 'rest' will start matching from,
            # i.e. 'rest' tries to match text[i:]
            #
            # if self.rest.match(text[i:]):
            #     return n
            # OR
            j = self.rest._match(text, start=i)
            if j == n: # NOTE2: why we test for this (hint we don't rely on 'rest')
                # success
                return n # NOTE3

        # fail
        return None
#+end_src

- NOTE :: In the above code
  - NOTE1
  - NOTE2
  - NOTE3

#+begin_src sh
uvx pytest ./src/ch4/test_any.py
#+end_src

#+RESULTS:
: =============================== test session starts ================================
: platform linux -- Python 3.12.8, pytest-8.4.1, pluggy-1.6.0
: rootdir: /home/nabeel/Documents/public/books/Software Design by Example
: collected 5 items
:
: src/ch4/test_any.py .....                                                    [100%]
:
: ================================ 5 passed in 0.01s =================================

*** Implement Either(Pattern)

#+name: test either
#+begin_src python :tangle ./src/ch4/test_either.py
from matcher import *

def test_either_two_literals_first():
    # /{a,b,c}/ matches "a"
    assert Either(Lit("a"), Lit("b")).match("a")

def test_either_two_literals_not_both():
    # /{a,b,c}/ doesn't match "ab"
    assert not Either(Lit("a"), Lit("b"), Lit("c")).match("ab")

def test_either_followed_by_literal_match():
    # /{a,b,c}d/ matches "cd"
    assert Either(Lit("a"), Lit("b"), Lit("c"), rest=Lit("d")).match("cd")

def test_either_followed_by_literal_no_match():
    # /{a,b,c}d/ doesn't match "cx"
    assert not Either(Lit("a"), Lit("b"), Lit("c"), rest=Lit("d")).match("cx")

def test_either_followed_by_literal_no_match2():
    # /{a,b,cd}d/ matches "cd"
    assert not Either(Lit("a"), Lit("b"), Lit("cd"), rest=Lit("d")).match("cd")

def test_empty_either_empty_literal_match():
    # /{}/ matches ""
    assert Either().match("")

def test_empty_either_literal_match():
    # /{}abc/ matches ""
    assert Either(rest=Lit("abc")).match("abc")

def test_empty_either_literal_no_match():
    # /{}abc/ doesn't match "abd"
    assert not Either().match("abd")
#+end_src

Keeping in mind the above test cases:

#+name: either pattern
#+begin_src python
class Either(Pattern):
    def __init__(self, *patterns, rest=None):
        super().__init__(rest)
        self.patterns = patterns

    def _match(self, text, start=0):
        # NOTE: what if patterns are empty ?
        if not self.patterns:
            return self.rest._match(text, start)

        # Try each pattern
        for pattern in self.patterns:
            j = pattern._match(text, start)
            if j is None:
                continue
            if len(text) == self.rest._match(text, start=j):
                # pass
                return len(text)
        # fail
        return None
#+end_src

#+begin_src sh
uvx pytest ./src/ch4/test_either.py
#+end_src

#+RESULTS:
: =============================== test session starts ================================
: platform linux -- Python 3.12.8, pytest-8.4.1, pluggy-1.6.0
: rootdir: /home/nabeel/Documents/public/books/Software Design by Example
: collected 8 items
:
: src/ch4/test_either.py ........                                              [100%]
:
: ================================ 8 passed in 0.01s =================================

* Chapter 5: Parsing Text
:PROPERTIES:
:ref: https://third-bit.com/sdxpy/parse/
:header-args:python: :session ch5 :results output :async yes :eval no
:header-args:sh: :session ch5-sh :results output :async yes
:END:

** Summary
:PROPERTIES:
:ID:       6e6a4a75-8d34-4f9c-ad32-7cb0d668fd09
:END:

#+caption: Gist
#+begin_example
Tranform text -> list of tokens List[[token_type, token_args]] --parse-> either AST or actual python objects
#+end_example

 - Parsing transforms text that's easy for people to read into objects that are easy for computers to work with.
 - A grammar defines the textual patterns that a parser recognizes.

 - Most parsers tokenize input text and then analyze the tokens.
 - Most parsers need to implement some form of precedence to prioritize different patterns.

 - Operations like addition and function call work just like user-defined functions.
 - Programs can overload built-in operators by defining specially-named methods that are recognized by the compiler or interpreter.

#+caption: Parser concept map
[[file:images/Chapter_5:_Parsing_Text/2025-08-19_16-39-33_screenshot.png]]

** Theory
*** Intro
We constructed objects to match patterns in [[*Chapter 4: Matching patterns][Chapter 4: Matching patterns]], but an expression like ~"2023-*{pdf,txt}"~ is a lot easier to read and write than code like ~Lit("2023-", Any(Either("pdf", "txt")))~.

If we want to use the former, we need a parser to convert those human-readable strings into machine-comprehensible objects.

#+caption: glob grammar that our parse will handle
| *Meaning*                 | *Character* |
|-------------------------+-----------|
| Any literal character c | c         |
| Zero or more characters | *         |
| Alternatives            | {x,y}     |

When we are done, our parser should be able to recognize that =2023-*.{pdf,txt}= means the literal =2023-= , any characters, a literal =.=, and then either a literal =pdf= or a literal =txt=.

*** Tokenizing
:PROPERTIES:
:ID:       13eb27ae-7cbf-4c1d-9671-a4d69fdb3aa2
:END:

Most parsers are written in 2 parts:

1. The first stage groups characters into atoms of text called “tokens“, which are meaningful pieces of text like the digits making up a number or the letters making up a variable name.

2. The second stage of parsing assembles tokens to create an *abstract syntax tree*,
   that represents the structure of what was parsed

#+caption: Stages in parsing pipeline
[[file:images/Chapter_5:_Parsing_Text/2025-08-20_10-08-30_screenshot.png]]

Our grammars token are:
- special characters:
  - ,
  - {
  - }
  - *
- (other characters) a sequence of one or more other characters is a single multi-letter token.

The above classification determines the design of our tokenizer:

1. If a character is not special, then append it to the current literal (if there is one) or start a new literal (if there isn’t).

2. If a character is special, then close the existing literal (if there is one) and create a token for the special character. Note that the =,= character closes a literal but doesn’t produce a token.

Eg: For the string "2023-*{pdf, txt}", out tokenizer will output:
#+begin_src python
[
    ['Lit', '2023-'],
    ['Any'],
    ['EitherStart'],
    ['Lit', 'pdf'],
    ['Lit', 'txt'],
    ['EitherEnd']
]
#+end_src

We use the above structure for our tokens because they represent the pattern and the arguments that pattern takes.

#+name: tokenizer
#+begin_src python :tangle ./src/ch5/tokenizer.py
class Tokenizer():
    def __init__(self):
        self._setup()

    def _setup(self):
        # NOTE: We are defining the class attributes in a function
        #       other than __init__ !
        self.result = []
        self.current = ""

    def _add(self, thing):
        """
        Adds the current thing to the list of tokens.
        Examples of 'thing': ['Any'], ['EitherStart'], ['EitherEnd']

        As a special case, self._add(None) means “add the literal but nothing
        else”
        """
        if len(self.current) > 0:
            self.result.append(['Lit', self.current])
            self.current = ''
        if thing is not None:
            self.result.append(thing)

    def tok(self, text: str):
        """
        Main method of our tokenizer
        """
        # This method calls self._setup() at the start so that the tokenizer can
        # be re-used
        self._setup()

        for c in text:
            if c == '*':
                self._add(['Any'])
            elif c == '{':
                self._add(['EitherStart'])
            elif c == '}':
                self._add(['EitherEnd'])
            elif c == ',':
                self._add(None)
            elif c.isascii():
                self.current += c
            else:
                raise NotImplementedError(f'what is {c} ?')
        # NOTE: We do this to add the final 'current' to the 'result'.
        return self.result
#+end_src

The above class based implementation is very nice:
- we have ~_setup()~ method to *reset* the state of our /current/ and /result/ variables.
- we have ~_add(pattern_name | None)~ to add the literal (before adding the special character, if it exists).

#+name: tokenizer tests
#+begin_src python :tangle ./src/ch5/test_tokenizer.py
from .tokenizer import *

def test_tok_empty_string():
    assert Tokenizer().tok("") == []

def test_tok_any_either():
    assert Tokenizer().tok("*{abc,def}") == [
        ["Any"],
        ["EitherStart"],
        ["Lit", "abc"],
        ["Lit", "def"],
        ["EitherEnd"],
    ]
#+end_src

#+begin_src sh
uvx pytest ./src/ch5/test_tokenizer.py
#+end_src

#+RESULTS:
: ============================================================================ test session starts ============================================================================
: platform linux -- Python 3.12.8, pytest-8.4.1, pluggy-1.6.0
: rootdir: /home/nabeel/Documents/public/books/Software Design by Example
: collected 2 items
:
: src/ch5/test_tokenizer.py ..                                                                                                                                          [100%]
:
: ============================================================================= 2 passed in 0.01s =============================================================================

*** Parsing

We now need to turn the list of tokens into a tree.

Just as we used a class for tokenizing, we will create one for parsing and give it a ~_parse~ method to start things off.
This method doesn’t do any conversion itself.
Instead, it takes a token off the front of the list and figures out which method handles tokens of that kind:

#+begin_src python :tangle ./src/ch5/parser.py
from .tokenizer import *
from ..ch4.matcher import *

class Parser():
    def __init__(self):
        pass

    def parse(self, text: str):
        tokens = Tokenizer().tok(text)
        return self._parse(tokens)

    def _parse(self, tokens):
        if not tokens:
            return Null()

        car = tokens[0]
        cdr = tokens[1:]
        pattern = car[0]
        if pattern == 'Any':
            handler = self._parse_Any
        elif pattern == 'Lit':
            handler = self._parse_Lit
        elif pattern == 'EitherStart':
            handler = self._parse_EitherStart
        else:
            assert False, f'Unknown token type {pattern}'

        return handler(car[1:], cdr)

    def _parse_Any(self, arg, rest_tokens):
        return Any(rest=self._parse(rest_tokens))

    def _parse_Lit(self, arg, rest_tokens):
        text = arg[0]
        return Lit(chars=text, rest=self._parse(rest_tokens))

    def _parse_EitherStart(self, arg, rest_tokens):
        args = [] # with store the options of Either

        for i in range(len(rest_tokens)):
            token = rest_tokens[i]
            if token[0] == 'EitherEnd':
                either_end_index = i
                break
            else:
                pattern = self._parse([token])
                args.append(pattern)

        return Either(*args, rest=self._parse(rest_tokens[either_end_index+1:]))
#+end_src

Now let's write a test to test our implementation of ~Parser~:

#+begin_src python :tangle ./src/ch5/test_parser.py
from .parser import *
from ..ch4.matcher import *

def test_parse_either_two_lit():
    assert Parser().parse("{abc,def}") == Either(
        [Lit("abc"), Lit("def")]
    )
#+end_src

To run the above test, we need to first implement *equal* operation by the ~Pattern~ class.

This test assumes we can compare ~Pattern~ objects using ====, just as we would compare numbers or strings. so we add a ~__eq__~ method to our classes:

#+name: Pattern __eq__
#+begin_src python
def __eq__(self, other):
    return (other is not None
            and self.__class__ == other.__class__
            and self.rest == other.rest)
#+end_src

#+name: Lit __eq__
#+begin_src python
def __eq__(self, other):
    return super().__eq__(other) and (self.chars == other.chars)

#+end_src

In the above Note:
- definition of ~__eq__(self, other)~
- usage of ~__class__~
- usage of ~super().__eq__(other)__~

- Since we’re using inheritance to implement our matchers, we write the check for equality in two parts.

  1. The parent class ~Pattern~ performs the checks that all classes need to perform (in this case, that the objects being compared have the same /concrete class/).

  2. If the child class needs to do any more checking (for example, that the characters in two ~Lit~ objects are the same) it calls up to the parent method first, then adds its own tests.

#+begin_src sh
uvx pytest ./src/ch5/test_parser.py
#+end_src

#+RESULTS:
: ============================================================================ test session starts ============================================================================
: platform linux -- Python 3.12.8, pytest-8.4.1, pluggy-1.6.0
: rootdir: /home/nabeel/Documents/public/books/Software Design by Example
: collected 1 item
:
: src/ch5/test_parser.py .                                                                                                                                              [100%]
:
: ============================================================================= 1 passed in 0.01s =============================================================================
** Questions
*** Nested Lists
*Q.* Write a function that accepts a string representing nested lists containing numbers and returns the actual list. For example, the input [1, [2, [3, 4], 5]] should produce the corresponding Python list.

*A.* convert the text into:
#+begin_src python
# [1, [2, [3, 4], 5]]
# is converted to
[
    ['ListStart'],
    ['Number', '1'],
    ['ListStart'],
    ['Number', '2'],
    ['ListStart'],
    ['Number', '3'],
    ['Number', '4'],
    ['ListEnd'],
    ['Number', '5'],
    ['ListEnd'],
    ['ListEnd'],
]
#+end_src


#+begin_src python :eval yes

def isnumber(text: str) -> bool:
    "Returns True if text represents a integer or decimal number"
    try:
        float(text)
        return True
    except:
        return False

def tokenize(text: str):
    current = ""
    result = []

    def add(token):
        """
        Add current to 'result', then adds 'token' to 'result'
        """
        nonlocal current
        if len(current) > 0:
            result.append(['Number', current])
            current = ''
        if token is not None:
            result.append(token)

    for char in text:
        if char == '[':
            # special tokens like these signal the end of 'current'
            # as well as need to be taken care of themselves
            add(['ListStart'])
        elif char == ']':
            add(['ListEnd'])
        elif char == ',':
            # ',' signals the end of 'current'
            add(None)
        elif char.isspace():
            # if i just ignore it
            # then [2 2] would represent [22],
            # but let's ignore that for now
            continue
        elif isnumber(current + char):
            current += char
        else:
            raise NotImplementedError(f'what is this char: {char} ?')

    add(None)
    return result

expected = [
    ['ListStart'],
    ['Number', '1'],
    ['ListStart'],
    ['Number', '2'],
    ['ListStart'],
    ['Number', '3'],
    ['Number', '4'],
    ['ListEnd'],
    ['Number', '5'],
    ['ListEnd'],
    ['ListEnd'],
]
actual = tokenize('[1, [2, [3, 4], 5]]')
print(actual == expected)
#+end_src

#+RESULTS:
: True

To parse this one has to process the list of tokens and call individual parse helper methods recursively.

*** Simple Arithmetic
*Q.*
Write a function that accepts a string consisting of numbers and the basic arithmetic operations +, -, *, and /, and produces a nested structure showing the operations in the correct order.

For example, 1 + 2 * 3 should produce ["+", 1, ["*", 2, 3]].

*A.* Here operations have a relative ordering, it is NOT simply left to right.

#+begin_src python
# For  "1 + 2 * 3"
# we get
[
    ['Number', '1'],
    ['+'],
    ['Number', '2'],
    ['*'],
    ['Number', '3'],
]
#+end_src

- NOTE :: Maybe we also have brackets "(1 + 2) * 3"

So, there is an ordering:
1. Brackets
2. Division
3. Multiplication
4. Addition
5. Subtraction

- Aim :: Is to convert this list of tokens into an AST, keeping in the above order of operations in mind.

Let's assume our string is:
#+begin_src python
example = '(3 + 3) / -.6 - -10 * 1.0'
#+end_src

In the above 'example', we have operations, numbers, and spaces.
- In this string, spaces shouldn't matter (assuming we have been given valid syntax)
- Therefore, our string only has number stuff(digits, decimal, and negative) and operations

#+name: tokenize arithmetic
#+begin_src python :tangle ./src/ch5/tokenize_arithmetic.py
from pprint import pprint

def tokenize(text: str):
    result = []
    current = ''

    def looking_for_number() -> bool:
        """
        Return true if 'result's last token is an operation
        """
        if not result:
            return False
        last_token = result[-1]
        # check if it is a number
        if last_token[0] == 'Number':
            return

    def add(token):
        nonlocal current
        if len(current) > 0:
            result.append(['Number', current])
            current = ''
        if token:
            result.append(token)

    for i, char in enumerate(text):
        if char == '(':
            add(['('])
        elif char == ')':
            add([')'])
        elif char == '+':
            add(['+'])
        elif char == '*':
            add(['*'])
        elif char == '/':
            add(['/'])
        elif char.isspace():
            continue
        # number can be '0-9',
        elif char.isdigit():
            current += char
        elif char == '.':
            if '.' not in current:
                current += char
            else:
                raise Exception(f'you promised valid string')
        elif char == '-':
            # now either this is the minus operation
            # or negative on a number
            #
            # If 'current' is empty => negative
            # If 'current' is not empty => minus operation
            if current:
                add(['-'])
            else:
                current += '-'
        else:
            raise Exception(f'provided text: {text} is supposed to be valid')

    add(None)
    return result
#+end_src

#+begin_src python :eval yes :noweb yes
<<tokenize arithmetic>>
example = '(3 + 3) / -.6 - -10 * 1.0'

pprint(tokenize(example))
#+end_src

#+RESULTS:
#+begin_example
[['('],
 ['Number', '3'],
 ['+'],
 ['Number', '3'],
 [')'],
 ['/'],
 ['Number', '-.6'],
 ['-'],
 ['Number', '-10'],
 ['*'],
 ['Number', '1.0']]
#+end_example

Now what remains is to convert this list of token into the requested AST.
To do that we will not parse left-to-right, but we will go through the tokens parsing (ruling out) each operation one by one.

#+name: parse arithmetic
#+begin_src python :noweb yes :tangle ./src/ch5/parse_arithmetic.py
from .tokenize_arithmetic import *

<<parsed arithmetic classes>>

<<handle bracket>>

<<handle division>>

<<handle multiplication>>

<<handle addition>>

<<handle subtraction>>

def parse_token(token) -> Parsed:
    if isinstance(token, Parsed):
        return token
    # either is an operation
    # or is a number
    if token[0] == 'Number':
        return Number(value=token[1])

    # token must be an operation
    # We can't parse an operation in isolation
    raise Exception(f'Cannot process operation: {token} in isolation')

def parse(tokens):
    tokens = handle_bracket(tokens)
    tokens = handle_division(tokens)
    tokens = handle_multiplication(tokens)
    tokens = handle_addition(tokens)
    tokens = handle_subtraction(tokens)
    return tokens


#+end_src

#+name: parsed arithmetic classes
#+begin_src python
from dataclasses import dataclass

class Parsed:
    pass

@dataclass
class Number(Parsed):
    value: str

    def __call__(self):
        return self.value

class Operation(Parsed):
    def __call__(self):
        return [self.op, self.left(), self.right()]

@dataclass
class Add(Operation):
    left: Parsed
    right: Parsed
    op: str = '+'

@dataclass
class Subtract(Operation):
    left: Parsed
    right: Parsed
    op: str = '-'

@dataclass
class Multiply(Operation):
    left: Parsed
    right: Parsed
    op: str = '*'

@dataclass
class Divide(Operation):
    left: Parsed
    right: Parsed
    op: str = '*'
#+end_src

#+name: handle bracket
#+begin_src python
def find_bracket_end(tokens, bracket_open_index):
    opened = 1
    for i in range(bracket_open_index+1, len(tokens)):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '(':
            opened += 1
        elif isinstance(token, list) and token[0] == ')':
            opened -= 1
            if opened == 0:
                return i

    raise Exception(f'Could not find corresponding closing bracket for open bracket at index: {bracket_open_index}')

def handle_bracket(tokens):
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '(':
            # we have to first parse everything inside this
            # '(' ... ')' pair
            # to find its bracket end
            j = find_bracket_end(tokens, i)
            tokens[i:j+1] = parse(tokens[i+1:j])
            #
            i = i
        else:
            i += 1
    return tokens
#+end_src

#+name: handle division
#+begin_src python
def handle_division(tokens):
    # a / b / c => (a / b) / c
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '/':
            left, right = tokens[i-1], tokens[i+1]
            tokens[i-1:i+2] = [Divide(left=parse_token(left),
                                     right=parse_token(right))]
            # 3 tokens replaced by 1
            # 0 1 2 3(/) 4
            # 0 1 2=23(/)4 3
            i = i
            continue
        else:
            i += 1

    return tokens
#+end_src

#+name: handle multiplication
#+begin_src python
def handle_multiplication(tokens):
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '*':
            left, right = tokens[i-1], tokens[i+1]
            tokens[i-1:i+2] = [Multiply(left=parse_token(left),
                                       right=parse_token(right))]
            i = i
            continue
        else:
            i += 1
    return tokens
#+end_src

#+name: handle addition
#+begin_src python
def handle_addition(tokens):
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '+':
            left, right = tokens[i-1], tokens[i+1]
            tokens[i-1:i+2] = [Add(left=parse_token(left),
                                  right=parse_token(right))]
            i = i
            continue
        else:
            i += 1
    return tokens
#+end_src

#+name: handle subtraction
#+begin_src python
def handle_subtraction(tokens):
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if isinstance(token, list) and token[0] == '-':
            left, right = tokens[i-1], tokens[i+1]
            tokens[i-1:i+2] = [Subtract(left=parse_token(left),
                                       right=parse_token(right))]
            i = i
            continue
        else:
            i += 1
    return tokens
#+end_src

Time to test it now:

#+begin_src python :tangle ./src/ch5/test_arithmetic.py
from .parse_arithmetic import *
from .tokenize_arithmetic import *
from pprint import pprint

def test_tokenize():
    example = '(3 + 3) / -.6 - -10 * 1.0'
    expected = [['('],
                ['Number', '3'],
                ['+'],
                ['Number', '3'],
                [')'],
                ['/'],
                ['Number', '-.6'],
                ['-'],
                ['Number', '-10'],
                ['*'],
                ['Number', '1.0']]
    actual = tokenize(example)
    assert actual == expected

def test_parse():
    example = '(3 + 3) / -.6 - -10 * 1.0'
    tokens = tokenize(example)
    actual = parse(tokens)
    expected = [Subtract(left=Divide(left=Add(left=Number(value='3'),
                                              right=Number(value='3'),
                                              op='+'),
                                     right=Number(value='-.6'),
                                     op='*'),
                         right=Multiply(left=Number(value='-10'),
                                        right=Number(value='1.0'),
                                        op='*'),
                         op='-')]
    assert actual == expected

def test_ast():
    example = '(3 + 3) / -.6 - -10 * 1.0'
    tokens = tokenize(example)
    parsed_list = parse(tokens)
    actual = parsed_list[0]()
    expected = ['-', ['*', ['+', '3', '3'], '-.6'], ['*', '-10', '1.0']]
    assert actual == expected

#+end_src

- NOTE :: ~@dataclass~ also implements equality, good for me
